{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c87c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# Always run this cell first!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import statsmodels.api # appear to need to import the api as well as the library itself for the interpreter to find the modules\n",
    "import statsmodels as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline\n",
    "plotly.offline.init_notebook_mode(connected=True) # make plotly work with Jupyter Notebook using CDN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea9db9",
   "metadata": {},
   "source": [
    "# Hypothesis Testing for Categorical Data\n",
    "\n",
    "## Statistical Decision Making: The Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658b43e",
   "metadata": {},
   "source": [
    "In the [last section](01_Foundations.ipynb), we saw how probabilty theory can help us to estimate how close a sample average is to the true average of a population using a confidence interval. \n",
    "\n",
    "In this section, we will look at another important tool in the statistician's repetoire, the statistical test of hypotheses. Hypothesis testing works much like the scientific method: we make a hypothesis about how the world works, construct an experiment to test it, gather data from the experiment, and then evaluate how closely that data fits the hypothesis.\n",
    "\n",
    "Let's look at this process in the context of an example.\n",
    "\n",
    "### The Wellcome Global Monitor Survey\n",
    "\n",
    "The Wellcome Global Monitor survey, which is part of the World Gallup Poll, asks people around the world about their views on topics related to science and health. The [most recent data](https://doi.org/10.5255/UKDA-SN-8466-2), as of writing, is from 2018 and is available under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/). The data consists of survey answers from over 140,000 people in 144 different countries, collected in person or by telephone.\n",
    "\n",
    "One of our goals in this chapter is to help you understand the statistical decision making process, and, through learning more, to have more confidence about how scientists move forward in their understanding. As a statistician and educator, I'm very interested in the level of trust people have in scientists and the scientific process. \n",
    "\n",
    "Hence, we are going to look today at the Wellcome Global Monitor's Trust in Scientists Index, which measures how much each respondent trusts scientists based on their answers to five of the questions in the survey:\n",
    "\n",
    "- \"How much do you trust scientists in your country?\"\n",
    "- \"In general, how much do you trust scientists to find out accurate information about the world?\"\n",
    "- \"How much do you trust scientists working in colleges/universities in this country to do their work with the intention of benefiting the public?\"\n",
    "- \"How much do you trust scientists working in colleges/universities in this country to be open and honest about who is paying for their work?\"\n",
    "- \"How much do you trust scientists working for companies in this country to do their work with the intention of benefiting the public?\"\n",
    "\n",
    "The answers are combined to rank each respondent's level of trust in scientists from \"High Trust\" to \"Low Trust\". Respondents that did not answer enough of the questions are recorded as \"No Score\".\n",
    "\n",
    "The survey data we have is contained in a file named `wellcome_global_monitor_2018.csv`. This file contains a subset of the attributes from the original dataset, replacing numerical codes with their textual equivalents. The column labeled `Trust_Index` (originally named \"WGM_Indexr\") contains the Trust in Scientists Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591d356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgm = pd.read_csv('wellcome_global_monitor_2018.csv')\n",
    "wgm.sample(5).head() # display 5 randomly sampled rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d085093",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's take a deeper look at the Trust in Scientists Index and see what levels of trust were recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6263601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wgm[\"Trust_Index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f70a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from counts of \"Trust_Index\" values\n",
    "levels = wgm[\"Trust_Index\"].value_counts().reset_index(name=\"Count\")\n",
    "\n",
    "# create bar plot\n",
    "ax = sns.barplot(\n",
    "    data = levels,\n",
    "    x = \"Trust_Index\",\n",
    "    y = \"Count\",\n",
    "    order = [\"No score\", \"Low trust\", \"Medium trust\", \"High trust\"]\n",
    ")\n",
    "\n",
    "# label each bar with the corresponding value\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "# set axis labels and title\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Trust in Scientists Index, Worldwide\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5051fe6",
   "metadata": {},
   "source": [
    "In what follows, we are going to narrow our analysis to focus in on one country. In this case, we will look at the United States. If you're interested in a different country, later on you'll get a chance to repeat this analysis for a different country in the exercises. Or, if you're using the interactive notebook, you can change the code to use a different country!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4789a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgm_usa = wgm[wgm['Country'] == \"United States\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b08824",
   "metadata": {},
   "source": [
    "Here's what the levels of trust for the sample from the United States look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160eb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgm_usa[\"Trust_Index\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77d61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from counts of \"Trust_Index\" values\n",
    "levels = wgm_usa[\"Trust_Index\"].value_counts().reset_index(name=\"Count\")\n",
    "\n",
    "# create bar plot\n",
    "ax = sns.barplot(\n",
    "    data = levels,\n",
    "    x = \"Trust_Index\",\n",
    "    y = \"Count\",\n",
    "    order = [\"No score\", \"Low trust\", \"Medium trust\", \"High trust\"]\n",
    ")\n",
    "\n",
    "# label each bar with the corresponding value\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "# set axis labels and title\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Trust in Scientists Index for the United States\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104114c",
   "metadata": {},
   "source": [
    "### Making a Hypothesis\n",
    "\n",
    "The first step in a hypothesis test is to determine the hypotheses that we are comparing. We are comparing two opposing views of the world: either it works like this, or it doesn't. These views are best understood in terms of *models*---a framework for understanding how the world works. \n",
    "\n",
    "For example, perhaps I think that the proportion of residents in the United States who have a high trust in scientists is large, say over 0.25. In this situation, the two models are statistical: we are hypothesizing about the proportion from a population. Either the proportion of people in our chosen country who have high trust in scientists is 0.25, or it's more than that. In mathematical terms, we'll write this as\n",
    "\n",
    "\\begin{equation}\n",
    "    H_0: p = 0.25, \\\\\n",
    "    H_1: p > 0.25.\n",
    "\\end{equation}\n",
    "\n",
    "In other words, our null hypothesis, $H_0$, is that the proportion is 0.25. Our alternative hypothesis, $H_1$, is that the proportion is greater than 0.25. Remember that the null hypothesis should always be in the form \"population parameter equals something\".\n",
    "\n",
    "We can make hypotheses about other parameters, such as mean or standard deviation, but we'll start here by working with proportions.\n",
    "\n",
    "The statistical hypothesis test is the method for making a decision between these two models. We gather data and see how well the data fits with what we would expect based on the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eace3b",
   "metadata": {},
   "source": [
    "### How Much Evidence Do We Need?\n",
    "\n",
    "After we have determined the hypotheses, the next step is to determine our threshold for making a decision. How much evidence do we need to reject our null hypothesis in favor of the alternative hypothesis? Another way to think about this is, how much risk are we willing to take to be wrong?\n",
    "\n",
    "In this case, there are actaully *two* ways to make the wrong conclusion: we could reject our null hypothesis when it is actually true (called a **Type I error**), or we could fail to reject the null hypothesis when it is actually false (a **Type II error**). We won't go any deeper into Type II error at the moment, but it is just as important to consider!\n",
    "\n",
    "The \"risk level\" or \"evidence threshold\" associated with the first type of wrong conclusion is denoted by the Greek letter $\\alpha$ (alpha). For any given example, the value should be chosen based on both the context of the problem and the risk of making a Type I error that you are willing to take. Typical values for $\\alpha$ in scientific literature are 0.1, 0.05, and 0.01, but these values are somewhat arbitrary. \n",
    "\n",
    "For instance, imagine you are trying to determine if a new medical treatment is more effective than a previous treatment. Suppose the new treatment has more potential for harmful side-effects than the previous treatment. In this case, you would want to be very sure the new treatment is better before you start using it widely, so you would want to set your $\\alpha$-value to something smaller to make sure that the risk of making a Type I error is not too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b89b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f56e3",
   "metadata": {},
   "source": [
    "### The Test Statistic: Using Data to Decide\n",
    "\n",
    "After determining the value of $\\alpha$, the next step of a real-world study would be to gather the data we need and make a decision. In the statistics classroom, you typically have the data or summary statistics already available to you, as we do in this case.\n",
    "\n",
    "We are asking questions about the proportion of people in the United States who have high trust in scientists. It would be impossible to get an answer from every single person in the entire country! However, the Wellcome Global Monitor data gives us answers from a large representative sample of people in the USA. So we can use this data to get a statistically sound idea of what the true proportion in the entire population is.\n",
    "\n",
    "Let's compare the proportion from the sample to what the null hypothesis says it should be. If the sample proportion is close to what the null hypothesis says it should be, that would mean that the null hypothesis is probably right. On the other hand, if it is different, perhaps the null hypothesis is wrong!\n",
    "\n",
    "We can compute the sample proportion of people with high trust in scientists by taking the total number of people in the sample with a high trust score and dividing it by the total sample size:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat p = \\frac{\\mbox{num. of respondents with high trust in scientists}}{\\mbox{num. of respondents}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306f196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of successes (i.e., people with high trust in scientists)\n",
    "count = len(wgm_usa[wgm_usa[\"Trust_Index\"] == \"High trust\"])\n",
    "\n",
    "# sample size (i.e., the total number of observations)\n",
    "nobs = len(wgm_usa[\"Trust_Index\"])\n",
    "\n",
    "# compute sample proportion\n",
    "p_hat =  count / nobs\n",
    "print(f\"The sample proportion is {p_hat:.3f}. There are {nobs} total observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb817cc",
   "metadata": {},
   "source": [
    "This is clearly more than $0.25$, the value in our null hypothesis! But remember that the sample proportion is variable, so it won't be *exactly* the same as the population proportion. So is this evidence for the alternative hypothesis?\n",
    "\n",
    "We could ask a better question: is the sample proportion of 0.28 a lot different from our null hypothesis or just a little bit different? We can answer this using probability theory. Just like in the last section, the theoretical foundation for the hypothesis test is the Central Limit Theorem. Remember that if the sample size $n$ is large enough, sample proportions have a normal distribution, where the mean is the population proportion $p$, and the standard deviation is the population standard deviation $\\sqrt{p(1-p)}$ divided by $\\sqrt{n}$:\n",
    "\n",
    "$$ \\hat{p} \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}} \\right). $$\n",
    "\n",
    "Above we can see that $n = 1006$. If we assume the null hypothesis is true, then we also know what to fill in for $p$:\n",
    "\n",
    "$$ \\hat{p} \\sim N\\left(.25, \\sqrt{\\frac{.25(1-.25)}{1006}} \\right). $$\n",
    "\n",
    "We call this the **null distribution**, since it is the distribution of the sample statistic under the assumption that the null hypothesis is true.\n",
    "\n",
    "Back to the question: is the difference between the observed value of $\\hat p$ and the value that the null hypothesis says it should be a big difference or small difference? One way to answer that question would be to **standardize** by taking the difference between the sample mean and the (hypothesized) population mean and dividing by the standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\frac{\\hat p - .25}{\\sqrt{\\frac{.25(1-.25)}{1006} }} \\sim N(0,1)\n",
    "\\end{equation}\n",
    "\n",
    "This gives us the standardized test statistic Z, which has a standard normal distribution ($\\mu = 0, \\sigma = 1$)---again assuming that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5043e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "z_obs = ( p_hat - 0.25 ) / np.sqrt( 0.25*(1-0.25) / nobs )\n",
    "print(f\"The observed value of the test statistic Z is {z_obs:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa88f10f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dictionary with properties of each distribution\n",
    "distributions = [\n",
    "    {\n",
    "        'name': 'Null Distribution of $\\hat{p}$',\n",
    "        'mean': 0.25, 'sd': np.sqrt(0.25*(1-0.25)/nobs),\n",
    "        'stat': p_hat, 'stat_label': '$\\hat{p}_{obs}$'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Null distribution of $Z$',\n",
    "        'mean': 0, 'sd': 1,\n",
    "        'stat': z_obs, 'stat_label': '$Z_{obs}$'\n",
    "    }\n",
    "]\n",
    "\n",
    "# create plot\n",
    "fig, axes = plt.subplots(1, len(distributions), figsize=(8,3))\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = iter(sns.color_palette())\n",
    "pdfcolor = next(palette)\n",
    "statcolor = next(palette)\n",
    "\n",
    "# for each distribution, plot the pdf, mean, and relevant statistic\n",
    "for ax, distr in zip(axes, distributions):\n",
    "    # plot pdf\n",
    "    x = np.linspace(distr['mean'] - 3.5*distr['sd'], distr['mean'] + 3.5*distr['sd'], 100)\n",
    "    ax.plot(x, scipy.stats.norm.pdf(x, distr['mean'], distr['sd']), color=pdfcolor, label='pdf')\n",
    "    # plot mean\n",
    "    ax.vlines(x=distr['mean'], ymin=0, ymax=scipy.stats.norm.pdf(distr['mean'], distr['mean'], distr['sd']), linestyle='dashed', color='grey', label='$\\mu$')\n",
    "    # plot statistic\n",
    "    ax.axvline(distr['stat'], linestyle='dashed', color=statcolor, label=distr['stat_label'])\n",
    "    # set y-axis limits to start at 0\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "    # set title\n",
    "    ax.set_title(distr['name'])\n",
    "    # display legend\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "axes[0].set_ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b306b13",
   "metadata": {},
   "source": [
    "This changes our difference to units of standard deviations. In other words, the sample proportion is 2.221 standard deviations above the mean value of 0. \n",
    "\n",
    "Think about it another way: if the null hypothesis is true, then most of the time $\\hat p$ will be close to $p=0.25$, and hence $Z$ will be close to 0. But $\\hat p$ varies, and hence $Z$ is variable. So how likely is it that $Z$ would be 2.221 standard deviations from what the null hypothesis says it should be most of the time? And how far away does it need to be before we would start to doubt the null hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f17be",
   "metadata": {},
   "source": [
    "### Making a Decision\n",
    "\n",
    "Now that we have a hypothesis and have calculated a test statistic to try to evaluate the hypothesis, we are ready to make a conclusion. Under the null hypothesis, we know the distribution of the sample statistic $\\hat p$. However, we have just one observation of the sample proportion. \n",
    "\n",
    "So we ask, if the null hypothesis is true and the population proportion really is 0.25, how likely is it that we would observe the value of 0.28 or something even further away from 0.25? Or translated to $Z$-space, how likely is it to get a test statistic that is 2.221 or larger? \n",
    "\n",
    "We can use `scipy.stats` to compute directly compute this probability from a $N(0,1)$ distribution using the CDF function. We subtract the probability from 1 since the CDF always gives the probability to the *left* of a number, and we want the probability that $Z$ is *larger* or to the right of 2.221."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f05f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - scipy.stats.norm(0,1).cdf(z_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d1bac",
   "metadata": {},
   "source": [
    "This value is called the $P$-value, and it measures the probability of observing a value at least as much greater than 0.25 as the value we actually observed is (assuming the null hypothesis is true). As the figure below shows, the $P$-value is the area under the probability density function of Z that is to the right of $Z_{obs}$. The smaller this area is, the less likely it is that, if the null hypothesis is indeed true and we took additional samples from our population, we would observe values of Z that are greater than or equal to $Z_{obs}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fcc1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph illustrating the P-value\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(8.5,4.5))\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = sns.color_palette()\n",
    "normcolor = palette[0]\n",
    "zcolor = palette[1]\n",
    "arrowcolor = palette[4]\n",
    "\n",
    "# plot standard normal pdf (pdf of Z)\n",
    "x = np.linspace(-4, 4, 150)\n",
    "y = scipy.stats.norm.pdf(x, 0, 1)\n",
    "ax.plot(x, y, color=normcolor, linewidth=2.5, label='pdf of Z')\n",
    "\n",
    "# set lower y-axis limit to 0\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "half_ylim = sum(ax.get_ylim())/2 # coordinate at middle of y-axis\n",
    "\n",
    "# plot and annotate Z_obs\n",
    "ax.axvline(z_obs, linestyle='dashed', linewidth=2, color=zcolor, label='$x = Z_{obs}$')\n",
    "\n",
    "# fill area under pdf represented by p-value\n",
    "ax.fill_between(x, y, where = (x >= z_obs), color=normcolor, alpha=0.5, label='area measured\\nby $P$-value')\n",
    "\n",
    "# annotate directions of increasing and decreasing likelihood\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = z_obs + 0.4,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = z_obs - 0.4,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "\n",
    "# set up legend, y-axis label, and title\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Likelihood of Observing $Z_{obs}$ (Or a More Extreme Value) Under $H_{0}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75776c8",
   "metadata": {},
   "source": [
    "In some sense, the $P$-value quantifies the evidence you have *against* the null hypothesis. If the $P$-value is small, then that means that if the null hypothesis is really true, then it is unlikely that you would get the value of $\\hat p$ that you did. In other words, a small $P$-value indicates that the data is in disagreement with the statistical model given by the null hypothesis.\n",
    "\n",
    "Based on the $P$-value, we can make our decision. A $P$-value is small enough if it is below the threshold of $\\alpha$ that we set previously. If the $P$-value is less than $\\alpha$, then our test statistic is incompatible with the null hypothesis, and we will **reject the null hypothesis**. But if the $P$-value is greater than or equal to $\\alpha$, then our data is compatible with the null hypothesis, and we will **fail to reject the null hypothesis**. \n",
    "\n",
    "In this case, we have chosen $\\alpha = 0.05$, and our $P$-value is 0.013. Since $0.013 < 0.05$, we will reject the null hypothesis.\n",
    "\n",
    "\n",
    "**Note:** Make sure you understand the difference between the population proportion $p$ and the $P$-value---they are very different! We chose to capitalize the $P$ in $P$-value to make this distinction more clear, but this is not the way you will find it in every book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ede3d1",
   "metadata": {},
   "source": [
    "In Python, we can put this all together using the `statsmodels` library, which we imported at the top of the notebook and gave the alias `sm`.\n",
    "\n",
    "A test for proportions using `statsmodels` is performed below by calling `sm.stats.proportion.proportions_ztest(...)`. The name `proportions_ztest` indicates that we will be using the standard normal distribution to model the test statistic. \n",
    "\n",
    "The arguments required by the test are all of the things we calculated above: the number of people with high trust in scientists (`count`), the sample size (`nobs`), and the hypothesized value of the proportion (`p0`). Because the alternative hypothesis is that $p > 0.25$, we add `alternative = \"larger\"`. Other possibilities for the alternative hypothesis are `\"smaller\"` or `\"two-sided\"`. The code returns the test statistic and the $P$-value associated with the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04bd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = 0.25 # proportion under null hypothesis\n",
    "(stat, pval) = sm.stats.proportion.proportions_ztest(\n",
    "    count = len(wgm_usa[wgm_usa[\"Trust_Index\"] == \"High trust\"]), # number of successes\n",
    "    nobs = len(wgm_usa[\"Trust_Index\"]), # number of observations\n",
    "    value = p0,\n",
    "    alternative = \"larger\",\n",
    "    prop_var = p0, # use proportion under null hypothesis to calculate variance for test statistic\n",
    ")\n",
    "print(f\"Z-test for proportion: test statistic is {stat:.3f}, P-value is {pval:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40fd9a",
   "metadata": {},
   "source": [
    "As you can see, `statsmodels` gives us the same test statistic and $P$-value that we found earlier. As before, since $0.013 < 0.05$, we reject the null hypothesis. Note that the output of the code doesn't tell you what conclusion to make. Remember again that $\\alpha$, the threshold for making your conclusion, is context-dependent, and the $P$-value is just one part of making a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47574089",
   "metadata": {},
   "source": [
    "## Difference of Two Proportions\n",
    "\n",
    "We can also use hypothesis testing to compare two proportions. This test is most common when you want to see if a population proportion is the same in two independent populations. For example, maybe we want to see if the proportion of men in the United States who have high trust in scientists ($p_1$) is different than the proportion of women in the United States who have high trust in scientists ($p_2$). Our two hypotheses are\n",
    "\n",
    "\\begin{equation}\n",
    "    H_0: p_1 - p_2 = 0, \\\\\n",
    "    H_1: p_1 - p_2 \\neq 0.\n",
    "\\end{equation}\n",
    "\n",
    "The main difference between this set of hypotheses and the previous set of hypotheses is that we aren't making a claim about whether the difference is bigger or smaller than a given value, just that it is different from that value. This will come into play later on when we quantify our evidence using the $P$-value.\n",
    "\n",
    "Just as before, we will use the sample proportions of men and women from our survey to make a determination between the two hypotheses. Let's take a look at these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77c85302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into two data sets\n",
    "wgm_m = wgm_usa[wgm_usa['Gender'] == \"Male\"]\n",
    "wgm_f = wgm_usa[wgm_usa['Gender'] == \"Female\"]\n",
    "\n",
    "# number of successes (i.e., people with high trust in scientists)\n",
    "count_m = len(wgm_m[wgm_m[\"Trust_Index\"] == \"High trust\"])\n",
    "count_f = len(wgm_f[wgm_f[\"Trust_Index\"] == \"High trust\"])\n",
    "\n",
    "# sample size (i.e., the total number of observations)\n",
    "nobs_m = len(wgm_m[\"Trust_Index\"])\n",
    "nobs_f = len(wgm_f[\"Trust_Index\"])\n",
    "\n",
    "# compute sample proportion\n",
    "p_hat_m =  count_m / nobs_m\n",
    "p_hat_f = count_f / nobs_f\n",
    "print(f\"The sample proportion for men is {p_hat_m:.3f}. There are {nobs_m} total observations for men.\")\n",
    "print(f\"The sample proportion for women is {p_hat_f:.3f}. There are {nobs_f} total observations for women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ecedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe\n",
    "levels = wgm_usa[[\"Gender\", \"Trust_Index\"]].value_counts().reset_index(name=\"Count\")\n",
    "\n",
    "# create bar plot\n",
    "ax = sns.barplot(\n",
    "    data = levels,\n",
    "    x = \"Trust_Index\",\n",
    "    y = \"Count\",\n",
    "    hue = \"Gender\",\n",
    "    order = [\"No score\", \"Low trust\", \"Medium trust\", \"High trust\"]\n",
    ")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Trust in Scientists Index for the United States\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a2a3e",
   "metadata": {},
   "source": [
    "Overall, in our sample we see a difference of about 0.04 between the proportions of men and women who have high trust in scientists. Again we ask, is that a large difference?\n",
    "\n",
    "Recall that if our sample sizes $n_1$ and $n_2$ are large enough,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat p_1 - \\hat p_2 \\sim N\\left(p_1 - p_2, \\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}  } \\right).\n",
    "\\end{equation}\n",
    "\n",
    "and hence\n",
    "\n",
    "\\begin{equation}\n",
    "    Z = \\frac{\\hat p_1 - \\hat p_2 - (p_1 - p_2)}{ \\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}  } } \\sim N(0,1).\n",
    "\\end{equation}\n",
    "\n",
    "The null hypothesis tells us what to fill in for $p_1-p_2$, but it doesn't hypothesize directly about the values of $p_1$ or $p_2$. However, it does hypothesize that $p_1 = p_2$, and it that is true, then our estimates $\\hat p_1$ and $\\hat p_2$ are estimating the same thing. In that case, we can combine them to estimate the variance. Define $p$ to be the common value of $p_1 = p_2 = p$. Then the variance of our test statistic is\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}  }  = \\sqrt{ \\frac{p(1-p)}{n_1} + \\frac{p(1-p)}{n_2}  } = \\sqrt{ p(1-p)\\left( \\frac{1}{n_1} + \\frac{1}{n_2}\\right)  },\n",
    "\\end{equation}\n",
    "\n",
    "and we can estimate $p$ using\n",
    "\n",
    "\\begin{equation}\n",
    "    p \\approx \\hat p = \\frac{\\mbox{num. of male respondents with high trust in scientists} + \\mbox{num. of female respondents with high trust in scientists}}{\\mbox{num. of male respondents} + \\mbox{num. of female respondents}}.\n",
    "\\end{equation}\n",
    "\n",
    "We call $\\hat p$ the **pooled sample proportion**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b71d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_hat = (count_m + count_f) / (nobs_m + nobs_f)\n",
    "print(f\"The pooled sample proportion is {p_hat:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b15d9",
   "metadata": {},
   "source": [
    "Now, using the formula for $Z$ above, we can calculate the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b794addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_obs = (p_hat_m - p_hat_f) / np.sqrt( p_hat*(1-p_hat)*(1/nobs_m + 1/nobs_f))\n",
    "print(f\"The observed value of the test statistic Z is {z_obs:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3fa50",
   "metadata": {},
   "source": [
    "Now that we have the test statistic, we can quantify the evidence that we have in relation to our two hypotheses. Since the alternative hypothesis is just interested in finding differences from 0, then values of $Z$ that are a lot above 0 *or* a lot below 0 would be evidence against the null hypothesis.\n",
    "\n",
    "In other words, if $p_1 - p_2$ really is 0, then $Z$ will be close to 0 most of the time. If it's not, then it is more likely that $Z$ is far from 0. We can quantify this through the $P$-value, which again is calculating the probability that we would observe a value a least as far from 0 as the one that we did. We double the value from the CDF here to get the $P$-value since \"far away\" includes both the positive and negative directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a52e870",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs_z_obs = abs(z_obs)\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(8.5,4.5))\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = sns.color_palette()\n",
    "normcolor = palette[0]\n",
    "zcolor = palette[1]\n",
    "arrowcolor = palette[4]\n",
    "\n",
    "# plot standard normal pdf (pdf of Z)\n",
    "x = np.linspace(-4, 4, 150)\n",
    "y = scipy.stats.norm.pdf(x, 0, 1)\n",
    "ax.plot(x, y, color=normcolor, linewidth=2.5, label='pdf of Z')\n",
    "\n",
    "# set lower y-axis limit to 0\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "half_ylim = sum(ax.get_ylim())/2 # coordinate at middle of y-axis\n",
    "\n",
    "# plot and annotate Z_obs and -Z_obs\n",
    "ax.axvline(z_obs, linestyle='dashed', linewidth=2, color=zcolor, label='$x = \\pm$ $Z_{obs}$')\n",
    "ax.axvline(-z_obs, linestyle='dashed', linewidth=2, color=zcolor)\n",
    "\n",
    "# fill area under pdf represented by p-value\n",
    "ax.fill_between(x, y, where = (x < -abs_z_obs) | (x > abs_z_obs), color=normcolor, alpha=0.5, label='area measured\\nby $P$-value')\n",
    "\n",
    "# annotate directions of increasing and decreasing likelihood\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = abs_z_obs + 0.4,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = abs_z_obs - 0.4,\n",
    "    y = half_ylim + 0.04,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = -abs_z_obs + 0.4,\n",
    "    y = half_ylim - 0.04,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = -abs_z_obs - 0.4,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "\n",
    "# set up legend, y-axis label, and title\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Likelihood of Observing $Z_{obs}$ (Or a More Extreme Value) Under $H_{0}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27532a5e",
   "metadata": {},
   "source": [
    "Let's see what this $P$-value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe7d35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "2*(scipy.stats.norm(0,1).cdf(z_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5480e3",
   "metadata": {},
   "source": [
    "This $P$-value is fairly large ($>0.05$), indicating that if the null hypothesis is true, it's not unlikely that we would observe a difference of 4% in our sample. So we fail to reject the null hypothesis because we do not find evidence in our data of a difference of high trust in scientists between men and women in the United States. \n",
    "\n",
    "## Using Python for the test\n",
    "\n",
    "Of course, we're here to see how to perform these tests using Python code! The function to perform the test is the same as the one from the previous section, except this time the arguments `count` and `nobs` each take a list containing the relevant information from both of our samples. We have changed the value of the argument `alternative` to account for the fact that we are performing a `two-sided` test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5c91e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p0 = 0 # proportion under null hypothesis\n",
    "(stat, pval) = sm.stats.proportion.proportions_ztest(\n",
    "    count = [\n",
    "        count_m, # number of successes for sample 1\n",
    "        count_f, # number of successes for sample 2\n",
    "    ],\n",
    "    nobs = [\n",
    "        nobs_m, # number of observations for sample 1\n",
    "        nobs_f, # number of observations for sample 2\n",
    "    ],\n",
    "    value = p0,\n",
    "    alternative = \"two-sided\",\n",
    "    prop_var = False, # use pooled sample proportion to calculate variance for test statistic\n",
    ")\n",
    "print(f\"Z-test for difference of two proportions: test statistic is {stat:.3f}, P-value is {pval:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf3917",
   "metadata": {},
   "source": [
    "Again, these values match up with what we did before, so the conclusion is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e89b69",
   "metadata": {},
   "source": [
    "## Further Exploration of P-Values\n",
    "\n",
    "To improve your understanding of one-sided and two-sided $P$-values and how they are related to the observed value of your test statistic, you can play around with the interactive widget below.\n",
    "\n",
    "Drag the slider to select different values of $Z_{obs}$. Compare the one-sided and two-sided $P$-values, and notice how the areas represented by the $P$-values shrink as $Z_{obs}$ gets further and further away from 0, the mean of the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b98a31d3",
   "metadata": {
    "scrolled": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Use default seaborn palette (reference: https://josephlemaitre.com/2022/06/use-a-seaborn-color-palette-for-plotly-figures/)\n",
    "plotly_palette = iter([f\"rgb({c[0]*256}, {c[1]*256}, {c[2]*256})\" for c in sns.color_palette()])\n",
    "normcolor = next(plotly_palette)\n",
    "pcolor = next(plotly_palette)\n",
    "\n",
    "# create figure with 3 subplots\n",
    "num_subplots = 3\n",
    "fig = go.Figure().set_subplots(\n",
    "    rows = num_subplots, cols = 1,\n",
    "    vertical_spacing = 0.1,\n",
    "    shared_xaxes = \"all\", shared_yaxes = \"all\",\n",
    "    subplot_titles = [\"One-Sided (Smaller)\", \"One-Sided (Greater)\", \"Two-Sided\"],\n",
    ")\n",
    "\n",
    "z_obs_values = np.arange(0.5, 2.75, 0.25)\n",
    "pvals = [1-scipy.stats.norm(0,1).cdf(z_obs) for z_obs in z_obs_values] # one-sided p-value\n",
    "num_z_obs = len(z_obs_values)\n",
    "initially_visible_index = 0 # index of initially visible z_obs and pval\n",
    "\n",
    "# for each z_obs\n",
    "for i, z_obs in enumerate(z_obs_values):\n",
    "    endpts = np.array([-4, -z_obs, z_obs, 4])\n",
    "    # for each subplot\n",
    "    for j in range(num_subplots):\n",
    "        # for each third of the pdf\n",
    "        for k in range(len(endpts)-1):\n",
    "            if (k == 0 and j != 1) or (k == 2 and j != 0):\n",
    "                # fill in the area under this third of the pdf\n",
    "                x = np.linspace(endpts[k], endpts[k+1], 50)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        line = {\"width\": 0, \"color\": normcolor,},\n",
    "                        opacity = 0,\n",
    "                        x = x,\n",
    "                        y = scipy.stats.norm.pdf(x),\n",
    "                        fill = \"tozeroy\",\n",
    "                        hoverinfo = \"skip\",\n",
    "                        showlegend = False,\n",
    "                        visible = True if i == initially_visible_index else False,\n",
    "                    ),\n",
    "                    row = j+1,\n",
    "                    col = 1\n",
    "                )\n",
    "        # add lines for x = -z_obs\n",
    "        if j != 1:\n",
    "            fig.add_vline(\n",
    "                x = -z_obs,\n",
    "                visible = True if i == initially_visible_index else False,\n",
    "                line = {\"color\": pcolor, \"dash\": \"dash\", \"width\": 3},\n",
    "                annotation_text = f\"$x = -{z_obs:.2f}$\",\n",
    "                annotation_visible = True if i == initially_visible_index else False,\n",
    "                annotation_borderpad = 5,\n",
    "                annotation_xshift = -5,\n",
    "                annotation_position = \"top left\",\n",
    "                annotation_font = {\"size\": 14 },\n",
    "                row = j+1,\n",
    "                col = 1\n",
    "            )\n",
    "        # add lines for x = z_obs\n",
    "        if j != 0:\n",
    "            fig.add_vline(\n",
    "                x = z_obs,\n",
    "                visible = True if i == initially_visible_index else False,\n",
    "                line = {\"color\": pcolor, \"dash\": \"dash\", \"width\": 3},\n",
    "                annotation_text = f\"$x = {z_obs:.2f}$\",\n",
    "                annotation_visible = True if i == initially_visible_index else False,\n",
    "                annotation_borderpad = 5,\n",
    "                annotation_position = \"top right\",\n",
    "                annotation_font = {\"size\": 14 },\n",
    "                row = j+1,\n",
    "                col = 1\n",
    "            )\n",
    "        # add text for p-value annotations\n",
    "        if j == 0:\n",
    "            text = f\"P-value = {pvals[i]:.3f}<br>= P(Z < -{z_obs_values[i]:.2f})\"\n",
    "        elif j == 1:\n",
    "            text = f\"P-value = {pvals[i]:.3f}<br>= P(Z > {z_obs_values[i]:.2f})\"\n",
    "        else:\n",
    "            text = f\"P-value = {2*pvals[i]:.3f}<br>= P(Z < -{z_obs_values[i]:.2f}<br>or Z > {z_obs_values[i]:.2f})\"\n",
    "        fig.add_annotation(\n",
    "            row = j+1, col = 1,\n",
    "            xref = \"x domain\", yref = \"y domain\",\n",
    "            x = 0.05, y = 0.45,\n",
    "            text = text,#f\"P-val &#8776; {pvals[i]:.3f}\" if j != 2 else f\"P-val &#8776; {2*pvals[i]:.3f}\",\n",
    "            align = \"left\",\n",
    "            showarrow = False,\n",
    "            font = {\"size\": 14},\n",
    "            xshift = 2,\n",
    "            yshift = 4,\n",
    "            visible = True if i == initially_visible_index else False,\n",
    "        ) \n",
    "\n",
    "    \n",
    "# add shapes for p-value annotations (always visible) to each subplot\n",
    "for i in range(num_subplots):\n",
    "    # filled-in square\n",
    "    fig.add_shape(\n",
    "        row = i+1, col = 1,\n",
    "        type = \"rect\",\n",
    "        xref = \"x domain\", yref = \"y domain\",\n",
    "        x0 = 0.028, x1 = 0.05, y0 = 0.4, y1 = 0.55,\n",
    "        fillcolor = normcolor,\n",
    "        opacity = 0.5,\n",
    "        visible = True,\n",
    "    )\n",
    "    # border for filled-in square\n",
    "    fig.add_shape(\n",
    "        row = i+1, col = 1,\n",
    "        type = \"rect\",\n",
    "        xref = \"x domain\", yref = \"y domain\",\n",
    "        x0 = 0.028, x1 = 0.05, y0 = 0.4, y1 = 0.55,\n",
    "        line = { \"color\": normcolor, \"width\": 3, },\n",
    "        visible = True,\n",
    "    )\n",
    "\n",
    "# add normal distribution (always visible) to each subplot\n",
    "x = np.linspace(endpts[0], endpts[-1], 100)\n",
    "for i in range(num_subplots):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            line = {\"width\": 4, \"color\": normcolor,},\n",
    "            x = x,\n",
    "            y = scipy.stats.norm.pdf(x),\n",
    "            showlegend = False,\n",
    "            legendrank = 0,\n",
    "            hoverinfo = \"x+y\",\n",
    "            visible = True,\n",
    "        ),\n",
    "        row = i+1,\n",
    "        col = 1\n",
    "    )  \n",
    "\n",
    "# add x = 0 (always visible) to each subplot\n",
    "for i in range(num_subplots):\n",
    "    fig.add_hline(y = 0, visible = True, line = {\"color\": \"grey\", \"dash\": \"dash\"}, row = i+1, col = 1)\n",
    "\n",
    "get_title = lambda i: f\"Likelihood of Observing |Z<sub>obs</sub>| = {z_obs_values[i]:.2f} (Or a More Extreme Value) Under H<sub>0</sub>\"\n",
    "\n",
    "def update_layout_options(index):\n",
    "    layout_options = { \"title\" : get_title(index) } # update title\n",
    "    # update visibility of shapes and annotations\n",
    "    for i in range(num_z_obs):\n",
    "        for offset in range(4):\n",
    "            # each group of 4 out of the first num_z_obs shapes represents the lines x = ±z_obs\n",
    "            # that correspond to z_obs_values[i]\n",
    "            layout_options[f\"shapes[{4*i + offset}].visible\"] = True if i == index else False\n",
    "        for offset in range(7):\n",
    "            # each group of 7 after the first 3 annotations represents the annotations for x = ±z_obs\n",
    "            # and for the p-value that correspond to z_obs_values[i]\n",
    "            layout_options[f\"annotations[{3 + 7*i + offset}].visible\"] = True if i == index else False\n",
    "    return layout_options\n",
    "\n",
    "# Create steps for the slider to select values of z_obs\n",
    "steps = []\n",
    "for i in range(num_z_obs):\n",
    "    steps.append({\n",
    "        \"method\": \"update\",\n",
    "        \"args\": [ # arguments to pass to the update method\n",
    "            {\n",
    "                \"visible\": np.append(np.ravel([[True]*4 if j == i else [False]*4 for j in range(num_z_obs)]), [True]*3), # trace arguments\n",
    "            },\n",
    "            update_layout_options(i), # layout arguments\n",
    "        ],\n",
    "        \"label\": f\"{z_obs_values[i]:.2f}\",\n",
    "    })\n",
    "\n",
    "# Create a slider using these steps and add it to fig\n",
    "fig.layout.sliders = [{\n",
    "    \"active\": initially_visible_index,\n",
    "    \"currentvalue\": {\"prefix\": \"|Z<sub>obs</sub>|: \", \"font\": {\"size\": 13}},\n",
    "    \"pad\": {\"t\": 50},\n",
    "    \"steps\": steps,\n",
    "}]\n",
    "\n",
    "# Set layout settings\n",
    "fig.update_layout(\n",
    "    title = {\n",
    "        \"x\": 0.5,\n",
    "        \"text\": get_title(initially_visible_index),\n",
    "    },\n",
    "    yaxis = {\n",
    "        \"range\": [-0.02, 0.45],\n",
    "        \"title\": \"Probability Density\",\n",
    "    },\n",
    "    paper_bgcolor = \"LightSteelBlue\",\n",
    "    height = 700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104758c8",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e14bf5",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "To get some practice, we are going to look at trust in scientists in a different country.\n",
    "\n",
    "1. Pick another country (perhaps your country of origin or another country you are interested in) and filter the observations to just include responses from that country. For example, here is how to do that for the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a94392d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wgm_usa = wgm[wgm['Country'] == \"United States\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89db20",
   "metadata": {},
   "source": [
    "2. Make a hypothesis about the proportion of all the people in your chosen country that have `High trust` in scientists. For example, do you think it is over 20%? Under 15%? Make any hypothesis you like.\n",
    "\n",
    "3. Choose a level of $\\alpha$.\n",
    "\n",
    "4. Compute the value of $\\hat p$.\n",
    "\n",
    "5. Run a hypothesis test using `statsmodels`.\n",
    "\n",
    "6. Write your conclusion in a complete sentence. Be sure to report the test statistic and the $P$-value. If you found a significant result, give a confidence interval for the proportion.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "1. Pick a second country of interest.\n",
    "\n",
    "2. Write hypotheses to test whether the proportion of people in the first country with high trust in scientists is higher than the same proportion in the second country.\n",
    "\n",
    "3. Choose a level of $\\alpha$.\n",
    "\n",
    "4. Run the hypothesis test using `statsmodels`.\n",
    "\n",
    "5. Write your conclusion in a complete sentence. Be sure to report the test statistic and the $P$-value. If you found a significant difference, give a confidence interval for the difference in proportions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
