{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c87c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# Always run this cell first!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import statsmodels.api # appear to need to import the api as well as the library itself for the interpreter to find the modules\n",
    "import statsmodels as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline\n",
    "plotly.offline.init_notebook_mode(connected=True) # make plotly work with Jupyter Notebook using CDN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea9db9",
   "metadata": {},
   "source": [
    "# Hypothesis Testing for Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658b43e",
   "metadata": {},
   "source": [
    "In the [last section](02_Inference_Categorical.ipynb), we learned about the statistical test of hypotheses, which tells us how we can choose between two statistical models. We did this in the context of categorical data, where we hypothesized about population proportions.\n",
    "\n",
    "In this section, we apply the same theory to numerical data to hypothesize about population means and variances.\n",
    "\n",
    "### The Palmer Penguins\n",
    "\n",
    "The [Palmer Penguins dataset](https://github.com/mcnakhaee/palmerpenguins), created by Allison Horst, Alison Hill, and Kristen Gorman, contains physical measurements for 344 adult penguins observed near the Palmer Station in Antarctica. The penguins come from 3 different species: Chinstrap, Gentoo, and Adélie. The data were collected by Kristen Gorman and the Palmer Station LTER Program.\n",
    "\n",
    "<figure>\n",
    "    <img src='https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png' alt='Illustration of Chinstrap, Gentoo, and Adélie penguins. Artwork by @allison_horst, used with permission.'>\n",
    "    <figcaption>Artwork by @allison_horst, used with permission.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To load the data, you need to make sure that the `palmerpenguins` package is installed in your environment. We'll load the data as a Pandas data frame named `penguins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aceebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "\n",
    "penguins = load_penguins()\n",
    "penguins.sample(5).head() # display 5 randomly sampled rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f33171",
   "metadata": {},
   "source": [
    "Using this data, we are going to explore the question of whether the Gentoo penguins at the Palmer Station differ significantly in body mass from the average Gentoo penguin body mass found in the scientific literature. We can find more information about the worldwide average body mass of the Gentoo penguin by using the [Encyclopedia of Life](https://eol.org/pages/45512076). Based on this page, it looks like the average body mass is 5500 grams. If we let $\\mu$ be the average body mass of Gentoo penguins in the Palmer Station area, our hypotheses are\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\mu = 5500, \\\\ H_1: \\mu \\neq 5500.\n",
    "\\end{equation}\n",
    "\n",
    "What about the mean body mass for penguins in our sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6ec131",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gentoo = penguins[penguins['species'] == 'Gentoo']\n",
    "sample_mean = gentoo['body_mass_g'].mean()\n",
    "print(f\"The mean Gentoo body mass in the sample data is {sample_mean:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4937f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://matplotlib.org/stable/gallery/ticks/tick-formatters.html\n",
    "from matplotlib import ticker\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.4, 0.4))\n",
    "\n",
    "# create number line by hiding unneeded elements of plot\n",
    "ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "ax.spines[['left', 'right', 'top']].set_visible(False)\n",
    "\n",
    "# set up number line limits\n",
    "ax.set_xlim(1000*((sample_mean-999)//1000), 1000*((sample_mean+1999)//1000))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# set up number line tick marks\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1000))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(100))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "# plot sample mean\n",
    "ax.scatter(sample_mean, 0, s = 75, clip_on=False, zorder=10)\n",
    "ax.annotate(\n",
    "    '$\\overline{X}$',\n",
    "    xy = (sample_mean, 0),\n",
    "    xytext = (-4, 10),\n",
    "    textcoords = 'offset pixels',\n",
    "    fontsize = 'large',\n",
    ")\n",
    "\n",
    "# plot population mean\n",
    "ax.scatter(5500, 0, s = 75, clip_on=False, zorder=10)\n",
    "ax.annotate(\n",
    "    '$\\mu$',\n",
    "    xy = (5500, 0),\n",
    "    xytext = (-4, 12),\n",
    "    textcoords = 'offset pixels',\n",
    "    fontsize = 'large',\n",
    ")\n",
    "\n",
    "ax.set_title('Sample Mean vs. Population Mean', y = 1.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de8ccd",
   "metadata": {},
   "source": [
    "Though we could introduce a test based on the standard normal distribution---as we did in the previous section---in the case of numerical data, we are going to utilize the **T-test** instead. As the name suggests, this test is based on the Student's T-distribution. We do this because the T-test will apply both to the cases when the sample size is large and to the cases when it is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e465c6",
   "metadata": {},
   "source": [
    "## The Student's T-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc90607",
   "metadata": {},
   "source": [
    "When hypothesizing about a population proportion, the null hypothesis gives us a value of $p$ that determines both the mean and standard deviation of the null distribution. On the other hand, the CLT for sample means tells us that as long as $n$ is large enough,\n",
    "\n",
    "$$\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}} \\right).$$\n",
    "\n",
    "Unfortunately, if we make a hypothesis about $\\mu$, we are still left with estimating $\\sigma$, the population standard deviation. If we have a good estimate for $\\sigma$, perhaps from past experiments or expert knowledge, then this is no problem! If we don't know $\\sigma$, our best estimate from the data will be to use $S$, the sample standard deviation.\n",
    "\n",
    "When we standardize the test statistic, we then have\n",
    "\n",
    "$$ T = \\frac{ \\overline{X} - \\mu } {S / \\sqrt{n}} \\sim t_{\\nu} \\qquad \\mbox{rather than} \\qquad Z = \\frac{ \\overline{X} - \\mu } {\\sigma / \\sqrt{n}} \\sim N(0,1).$$ (t-equation)\n",
    "\n",
    "The good news is that $T$ has a highly studied distribution called the Student's T-distribution. The T-distribution looks a lot like the standard normal distribution, but it has a wider spread, i.e., more probability in the \"tails\" of the distribution. This wider spread reflects the extra uncertainty of using the sample standard deviation $S$ in place of the population standard deviation $\\sigma$. The T-distribution has only one parameter, called \"degrees of freedom\", which is represented by the Greek letter $\\nu$ (*nu*, pronounced like \"new\"). This is calculated by subtracting one from the sample size, i.e., $\\nu = n - 1$.\n",
    "\n",
    "We have glossed over an important detail here: The statistic $T = \\frac{ \\overline{X} - \\mu } {S / \\sqrt{n}}$ has a T-distribution only if the data in the sample come from a normal distribution. Fortunately, if the sample size is large, we don't need to worry too much about this assumption thanks to the CLT. But if the sample size is small, this becomes especially important! \n",
    "\n",
    "\n",
    "Play around with the widget below to get an idea of what the T-distribution looks like compared to the standard normal distribution. Drag the slider to select different degrees of freedom, and click the labels in the legend to hide or show different parts of the graph. Notice how the T-distribution resembles the standard normal distribution more closely as the degrees of freedom increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcfdc61",
   "metadata": {
    "scrolled": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "degrees_freedom = np.arange(1, 32, 1)\n",
    "x = np.linspace(-4, 4, 100)\n",
    "initial_df_index = 0 # index of degrees of freedom to show initially\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Use default seaborn palette (reference: https://josephlemaitre.com/2022/06/use-a-seaborn-color-palette-for-plotly-figures/)\n",
    "plotly_palette = iter([f\"rgb({c[0]*256}, {c[1]*256}, {c[2]*256})\" for c in sns.color_palette()])\n",
    "normcolor = next(plotly_palette)\n",
    "tcolor = next(plotly_palette)\n",
    "\n",
    "# Add trace for standard normal distribution (always visible)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        visible = True,\n",
    "        line = { \"width\": 4 },\n",
    "        name = \"standard normal<br>distribution\", # label for legend\n",
    "        hoverinfo = \"x+y\",\n",
    "        marker = { \"color\": normcolor },\n",
    "        opacity = 0.7,\n",
    "        x = x,\n",
    "        y = scipy.stats.norm.pdf(x)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add traces for t-distributions\n",
    "for df in degrees_freedom:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible = True if df == degrees_freedom[initial_df_index] else False,\n",
    "            line = { \"width\": 4 },\n",
    "            name = f\"T-distribution<br>(df = {df})\", # label for legend\n",
    "            hoverinfo = \"x+y\",\n",
    "            marker = { \"color\": tcolor },\n",
    "            opacity = 0.7,\n",
    "            x = x,\n",
    "            y = scipy.stats.t.pdf(x, df)\n",
    "        )\n",
    "    )\n",
    "        \n",
    "# Create steps for the slider to select degrees of freedom\n",
    "steps = []\n",
    "for df in degrees_freedom:\n",
    "    steps.append({\n",
    "        \"method\": \"update\",\n",
    "        \"args\": [ # arguments to pass to the update method\n",
    "            # make only the normal distribution and the t-distribution with df degrees of freedom visible\n",
    "            { \"visible\": [True] + [True if i == df else False for i in degrees_freedom] },\n",
    "        ],\n",
    "        \"label\": f\"{df}\",\n",
    "    })\n",
    "\n",
    "# Create a slider using these steps and add it to fig\n",
    "fig.layout.sliders = [{\n",
    "    \"active\": initial_df_index,\n",
    "    \"currentvalue\": {\"prefix\": \"Degrees of Freedom: \"},\n",
    "    \"pad\": {\"t\": 50},\n",
    "    \"steps\": steps,\n",
    "}]\n",
    "\n",
    "# Set layout settings\n",
    "fig.update_layout(\n",
    "    title = {\n",
    "        \"x\": 0.5,\n",
    "        \"text\": \"T-Distribution vs. Standard Normal Distribution\"\n",
    "    },\n",
    "    yaxis = {\n",
    "        \"title\": \"Probability Density\",\n",
    "        #\"tickformat\": \".2\",\n",
    "    },\n",
    "    paper_bgcolor = \"LightSteelBlue\"\n",
    ")\n",
    "\n",
    "# Display figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e8295",
   "metadata": {},
   "source": [
    "When $n$ is large ($\\ge 30$ or so), then $S$ should be close to $\\sigma$, and the T-distribution more or less resembles the standard normal $Z$. So why use T at all? It turns out that the T-distribution can also be useful in cases when we do not have a large enough sample size to use the normal distribution! In this case, as we've already mentioned, we do need one extra assumption: that the population distribution has a normal distribution. This ensures that the sample average will also have a normal distribution. We'll come back to this with a small sample example later in the section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d06c6",
   "metadata": {},
   "source": [
    "## The T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f48f6",
   "metadata": {},
   "source": [
    "Now back to our current example: to use a T-test, we need to check the assumption that the underlying data come from a normal distribution. We have $n=119$ observations, which means we're working with a large sample case. Additionally, the histogram of the body mass of the Gentoo penguins is roughly mound-shaped, so using the T-test is appropriate for this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8222a4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "ax = sns.histplot(\n",
    "    data = gentoo,\n",
    "    x = 'body_mass_g',\n",
    ")\n",
    "ax.set_xlabel('Body Mass (g)')\n",
    "ax.set_title('Distribution of Penguin Body Mass');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222c69e",
   "metadata": {},
   "source": [
    "Now since we know the values of $\\overline X$, $S$, and $n$, and the null hypothesis tells us what we assume $\\mu$ to be, we can form our test statistic $T$ based on the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb31a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gentoo['body_mass_g'].dropna() # drop missing values\n",
    "sample_mean = sample.mean()\n",
    "sample_sd = sample.std()\n",
    "n = len(sample)\n",
    "mu_0 = 5500 # hypothesized mean from H_0\n",
    "\n",
    "T_obs = (sample_mean - mu_0) / (sample_sd / np.sqrt(n))\n",
    "\n",
    "print(f\"The observed value of the test statistic T is {T_obs:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8dc1af",
   "metadata": {},
   "source": [
    "If the population mean really is 5500 g, then we would expect $\\overline X$ to be close to 5500 most of the time. Hence, we would expect $T$ to be close to 0 most of the time. \n",
    "\n",
    "On the other hand, if the null hypothesis is not true, then we would expect $\\overline X$ to be different than 5500, and hence $T$ will likely be much different from 0. Since we made a two-sided hypothesis, values of $T$ that are much bigger than 0 *and* values of $T$ much lower than 0 are both evidence against the null hypothesis.\n",
    "\n",
    "We observed a very low value of $T$ at -9.365. But again, we only get to observe one value of $T$ and not the entire distribution! Is -9.365 different enough from 0 that we should start doubting that the null hypothesis is true? We can state this question as a probability: If the null hypothesis is really true, what is the chance that we would observe a value of $T$ that is at least as far from 0 as the value we got (-9.365)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95050b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t_obs = abs(T_obs)\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(8.5,4.5))\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = sns.color_palette()\n",
    "pdfcolor = palette[0]\n",
    "tcolor = palette[1]\n",
    "arrowcolor = palette[4]\n",
    "\n",
    "# define x-axis limits\n",
    "xlim = (-abs_t_obs-10, abs_t_obs+10)\n",
    "\n",
    "# plot t-distribution pdf (pdf of T)\n",
    "x = np.linspace(*xlim, 300)\n",
    "y = scipy.stats.t.pdf(x, df=n-1)\n",
    "ax.plot(x, y, color=pdfcolor, linewidth=2.5, label='pdf of T')\n",
    "\n",
    "# set axis limits\n",
    "ax.set_xlim(*xlim)\n",
    "half_ylim = sum(ax.get_ylim())/2 # coordinate at middle of y-axis\n",
    "\n",
    "# plot T_obs and -T_obs\n",
    "ax.axvline(T_obs, linestyle='dashed', linewidth=2, color=tcolor, label='$x = \\pm T_{obs}$')\n",
    "ax.axvline(-T_obs, linestyle='dashed', linewidth=2, color=tcolor)\n",
    "\n",
    "# plot x = 0\n",
    "ax.axhline(0, linestyle='dashed', linewidth=2, color='grey', label='$x = 0$')\n",
    "\n",
    "# fill area under pdf represented by p-value\n",
    "ax.fill_between(x, y, where = (x < -abs_t_obs) | (x > abs_t_obs), color=pdfcolor, alpha=0.5, label='area measured\\nby $P$-value')\n",
    "\n",
    "# annotate directions of increasing and decreasing likelihood\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = abs_t_obs + 1,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = abs_t_obs - 1,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = -abs_t_obs + 1,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = -abs_t_obs - 1,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "\n",
    "# set up legend, y-axis label, and title\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Likelihood of Observing $T_{obs}$ (Or a More Extreme Value) Under $H_{0}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc019131",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * scipy.stats.t.cdf(T_obs, df=n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a84fc9",
   "metadata": {},
   "source": [
    "This is the $P$-value again! Remember how we use the $P$-value to make a decision: If the $P$-value is greater than our specified threshold $\\alpha$, then our data is compatible with the null hypothesis, and we **fail to reject the null hypothesis**. If the $P$-value is less than our specified threshold $\\alpha$, then our test statistic is incompatible with the null hypothesis, and we will **reject the null hypothesis**.\n",
    "\n",
    "In this case, the $P$-value is very small. It's so small, in fact, that the area measured by the $P$-value in the  figure above---the area between the density function and the line $x = 0$ that is to the left of $-T_{obs}$ or to the right of $T_{obs}$---isn't even visible at this scale.\n",
    "\n",
    "Let's zoom in and take a closer look at this area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddab9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t_obs = abs(T_obs)\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(8.5,4.5))\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = sns.color_palette()\n",
    "pdfcolor = palette[0]\n",
    "tcolor = palette[1]\n",
    "arrowcolor = palette[4]\n",
    "\n",
    "# define x-axis limits\n",
    "xlim = (-abs_t_obs-10, abs_t_obs+10)\n",
    "\n",
    "# plot t-distribution pdf (pdf of T)\n",
    "x = np.linspace(*xlim, 300)\n",
    "y = scipy.stats.t.pdf(x, df=n-1)\n",
    "ax.plot(x, y, color=pdfcolor, linewidth=2.5, label='pdf of T')\n",
    "\n",
    "# set axis limits\n",
    "ax.set_xlim(*xlim)\n",
    "ax.set_ylim(-1e-16, scipy.stats.t.pdf(T_obs, df=n-1))\n",
    "half_ylim = sum(ax.get_ylim())/2 # coordinate at middle of y-axis\n",
    "\n",
    "# plot T_obs and -T_obs\n",
    "ax.axvline(T_obs, linestyle='dashed', linewidth=2, color=tcolor, label='$x = \\pm T_{obs}$')\n",
    "ax.axvline(-T_obs, linestyle='dashed', linewidth=2, color=tcolor)\n",
    "\n",
    "# plot x = 0\n",
    "ax.axhline(0, linestyle='dashed', linewidth=2, color='grey', label='$x = 0$')\n",
    "\n",
    "# fill area under pdf represented by p-value\n",
    "ax.fill_between(x, y, where = (x < -abs_t_obs) | (x > abs_t_obs), color=pdfcolor, alpha=0.5, label='area measured\\nby $P$-value')\n",
    "\n",
    "# annotate directions of increasing and decreasing likelihood\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = abs_t_obs + 1,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = abs_t_obs - 1,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'more likely',\n",
    "    x = -abs_t_obs + 1,\n",
    "    y = half_ylim,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'rarrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "ax.text(\n",
    "    s = 'less likely',\n",
    "    x = -abs_t_obs - 1,\n",
    "    y = half_ylim,\n",
    "    ha='right',\n",
    "    va='center',\n",
    "    size = 12,\n",
    "    bbox = { 'boxstyle': 'larrow', 'facecolor': 'thistle', 'edgecolor': arrowcolor, 'linewidth': 2 }\n",
    ")\n",
    "\n",
    "# set up legend, axis labels and tickmarks, and title\n",
    "ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:.0e}')) # use exponent notation\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Likelihood of Observing $T_{obs}$ (Or a More Extreme Value) Under $H_{0}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0e265",
   "metadata": {},
   "source": [
    "As this graph shows, the area measured by the $P$-value is quite small.\n",
    "\n",
    "Since the $P$-value is so small, we choose to reject the null hypothesis. There is evidence that the Gentoo penguins in this area are smaller than the average Gentoo penguin.\n",
    "\n",
    "Why the big difference? Probably the Encyclopedia of Life body mass trait is specifically for adult Gentoo penguins, and our observations contain younger penguins. If we wanted to make a stronger, publishable conclusion we could use this as a jumping off point for further research.\n",
    "\n",
    "**If we were writing this test for a publication we could use language like this: \"We conducted a one-sample t-test to compare the body mass of Gentoo penguins measured in the field with the theoretical mean adult Gentoo body mass of 5500 g pulled from the Encyclopedia of Life trait bank. The penguin body mass measurements in this study were significantly smaller than the theoretical mean (p=6.05x10^-16).\"**\n",
    "\n",
    "\n",
    "\n",
    "### The T-Test in Python\n",
    "\n",
    "Fortunately, Python has many of these statistical tests built in, so we don't have to calculate it all from scratch every time. In this case, we use the `scipy.stats` library, which includes the T-test as `ttest_1samp`. Here is code that performs the same test we just did. Verify that the answers match what we computed above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da037ee5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = scipy.stats.ttest_1samp(\n",
    "    a = sample,\n",
    "    popmean = 5500, # null hypothesis: mean is 0\n",
    "    alternative = 'two-sided',\n",
    ")\n",
    "print(f\"T-test for mean: test statistic is {result.statistic:.3f} with {n-1} degrees of freedom.\\nP-value is {result.pvalue:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42483b",
   "metadata": {},
   "source": [
    "The test again says that there is evidence that the population mean really is different from 5500. So what value will it take? To answer that, we can return to the confidence interval, this time calculated based on the T-distribution.\n",
    "\n",
    "Run the following code to check the version of `scipy` that you have installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f89c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d0639",
   "metadata": {},
   "source": [
    "If your version of `scipy` is 1.10.0 or greater, you can compute the confidence interval by using the `confidence_interval` function associated with the result of the t-test we just ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf9c762",
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# works for versions of scipy >= 1.10.0\n",
    "confidence_level = 0.95\n",
    "(lower_ci, upper_ci) = result.confidence_interval(confidence_level) # use the result object from the t-test\n",
    "print(f\"95% confidence interval is ({lower_ci:.3f}, {upper_ci:.3f}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7ba64",
   "metadata": {},
   "source": [
    "If your version of `scipy` is less than 1.10.0, you can compute the confidence interval by using the `t.interval` function instead. (It also works for versions greater than or equal to 1.10.0.) This function is similar to the `norm.interval` function we used [earlier](01_Foundations.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e87648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works for versions of scipy < 1.10.0\n",
    "confidence_level = 0.95\n",
    "(lower_ci, upper_ci) = scipy.stats.t.interval(\n",
    "    confidence_level,\n",
    "    df = n - 1, # degrees of freedom of test statistic\n",
    "    loc = sample_mean, # mean of test statistic\n",
    "    scale = sample_sd/np.sqrt(n) # standard deviation of test statistic\n",
    ")\n",
    "print(f\"95% confidence interval is ({lower_ci:.3f}, {upper_ci:.3f}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2457caa",
   "metadata": {},
   "source": [
    "We have 95% confidence that the average mass of Gentoo penguins in the Palmer Station area is between about 4986 and 5165 grams, much lower than the null hypothesis from before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8e39a",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Repeat the test above for the bill length of Gentoo penguins. This is found in the variable `bill_length_mm`.\n",
    "\n",
    "1. Find the average bill length for Gentoo penguins from the [Encyclopedia of Life](https://eol.org/pages/45512076). Use this information to form two hypotheses, making sure to note the units.\n",
    "2. What is the sample size you have? Does the distribution of bill lengths appear to meet the normality assumption needed to perform the T-test?\n",
    "3. Perform the test and report the test statistic and $P$-value.\n",
    "4. What do you conclude? If you find a significant difference, give a confidence interval for average bill length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff48f4",
   "metadata": {},
   "source": [
    "## Two-Sample T-Test\n",
    "\n",
    "We can also use the T-test to examine the difference between two population means. Typically, this is done to find out if two populations have the same mean or not.\n",
    "\n",
    "We can return to our Palmer Penguins example and look at whether the Adélie and Chinstrap penguins from this area have the same average body mass.\n",
    "\n",
    "Letting $\\mu_1$ be the average body mass of Adélie penguins and $\\mu_2$ be the average body mass of Chinstrap penguins in the Palmer station area, our hypotheses are\n",
    "\n",
    "\\begin{equation}\n",
    "H_0: \\mu_1 - \\mu_2 = 0, \\\\ H_1: \\mu_1 - \\mu_2 \\neq 0.\n",
    "\\end{equation}\n",
    "\n",
    "Let's look at our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c077fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adelie = penguins[penguins['species'] == 'Adelie']\n",
    "chinstrap = penguins[penguins['species'] == 'Chinstrap']\n",
    "\n",
    "print(f\"The mean Adélie body mass in the sample data is {adelie['body_mass_g'].mean():.3f}.\")\n",
    "print(f\"The mean Chinstrap body mass in the sample data is {chinstrap['body_mass_g'].mean():.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf29d3",
   "metadata": {},
   "source": [
    "These are slightly different, but are they enough different for us to reject the null hypothesis? We need a test statistic to quantify this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec82a1",
   "metadata": {},
   "source": [
    "### The Test Statistic\n",
    "\n",
    "When the sample sizes $n_1$ and $n_2$ are large enough (typically both $n_1 \\geq 30$ and $n_2 \\geq 30$), we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\overline{X}_1 - \\overline{X}_2 \\sim N \\left( \\mu_1 - \\mu_2, \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } \\right),\n",
    "\\end{equation}\n",
    "\n",
    "and hence\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} }} \\sim N(0,1).\n",
    "\\end{equation}\n",
    "\n",
    "Just as before, the null hypothesis will tell us about $\\mu_1-\\mu_2$, but not about $\\sigma_1$ and $\\sigma_2$, so we'll estimate them from the data using $S_1$ and $S_2$. Making that substitution for the standard deviations, we have a T-distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{ \\sqrt{ \\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2} }} \\sim t_{\\nu},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\nu$ represents the degrees of freedom. The exact value of $\\nu$ will be calculated by something called Welch's method, but we can let the software handle that details of the computation. You can keep in mind the intuition that if $S_1 \\approx S_2$, then $\\nu$ will be about $n_1 + n_2 - 2$. On the other hand, if $S_1$ and $S_2$ are very different, then $\\nu \\approx \\min({n_1, n_2})$.\n",
    "\n",
    "Now that we have two separate samples, the T-distribution requires that *both* come from normal distributions. There are sophisticated ways to check this, but we will just visually assess the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e41292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of adelie and chinstrap body mass, faceted by species\n",
    "facetgrid = sns.displot(\n",
    "    data = pd.concat([adelie, chinstrap]),\n",
    "    x = 'body_mass_g',\n",
    "    hue = 'species',\n",
    "    col = 'species',\n",
    "    kind = 'hist',\n",
    "    stat = 'density',\n",
    "    legend = False,\n",
    ")\n",
    "facetgrid.set_titles('Distribution of {col_name} Body Mass')\n",
    "facetgrid.set_xlabels('Body Mass (g)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de866d6e",
   "metadata": {},
   "source": [
    "These look appropriately normal-ish, and the large sample gives us extra wiggle room in the assumptions of the T-test. Moreover, the distributions don't look too different, so we might start to expect that the true difference in means really is 0. So let's perform the test!\n",
    "\n",
    "The function we will use is a function from `statsmodels` called `ttest_ind`. To use this function, we first need to create a `CompareMeans` object from our two samples. After we've done that, we will be able to call the `ttest_ind` function on this object to perform the actual T-test.\n",
    "\n",
    "We set the argument `value` equal to the difference of means under our null hypothesis. We set the argument `alternative` to \"two-sided\" to indicate that we have an two-sided alternative hypothesis. (Other possible values are \"larger\" and \"smaller\".) Finally, we set the argument `usevar` to \"unequal\" to make sure that Welch's method is used to calculate the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0cd94ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up our two samples (removing all observations with missing values)\n",
    "sample1 = adelie['body_mass_g'].dropna()\n",
    "sample2 = chinstrap['body_mass_g'].dropna()\n",
    "\n",
    "# create a CompareMeans object from the two samples\n",
    "cm = sm.stats.weightstats.CompareMeans.from_data(sample1, sample2)\n",
    "\n",
    "# perform the two-sample t-test\n",
    "(stat, pval, df) = cm.ttest_ind(\n",
    "    value = 0, # null hypothesis: difference in means is 0\n",
    "    alternative = 'two-sided', # two-sided alternative hypothesis\n",
    "    usevar = 'unequal', # perform Welch's t-test\n",
    ")\n",
    "\n",
    "print(f'Results of T-test: test statistic is {stat:.3f} with {df:.3f} degrees of freedom.\\nP-value is {pval:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2c784",
   "metadata": {},
   "source": [
    "We do not find any evidence of a difference in average body mass between these two species, so we fail to reject the null hypothesis.\n",
    "\n",
    "It's not as important in the situation where we've failed to reject the null hypothesis, but here is how to compute a confidence interval with this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27e7df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflevel = 0.95\n",
    "\n",
    "# compute confidence interval, using the same CompareMeans object as before\n",
    "(lower, upper) = cm.tconfint_diff(\n",
    "    alpha = 1-conflevel,\n",
    "    alternative = 'two-sided',\n",
    "    usevar = 'unequal', # perform Welch's t-test\n",
    ")\n",
    "print(f'{int(conflevel*100)}% confidence interval is ({lower:.3f}, {upper:.3f}).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b6fbf",
   "metadata": {},
   "source": [
    "Note that this confidence interval contains 0. Since we have 95% confidence that the difference in the average mass of Adélie and Chinstrap penguins in the Palmer Station area is in this interval, this confirms our conclusion that we have no evidence to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d012d",
   "metadata": {},
   "source": [
    "### Large Sample vs Small Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85a808",
   "metadata": {},
   "source": [
    "Why do we need the extra assumption of a normally distributed population in the case of a small sample? In non-technical terms, because we have less information in the data, we need more information from elsewhere (in this case, assumptions) to be able to make conclusions.\n",
    "\n",
    "In the large sample case, the Central Limit Theorem tells us that $\\overline{X}$ is normal. In the small sample case, if the sample comes from a normally distributed population, then $\\overline{X}$ will again be normal.\n",
    "\n",
    "Below you can find plots of the sampling distribution of sample averages from both normal and non-normal distributions, each with a population mean of 1 and a standard deviation of 2. The top row looks at small samples (size $n=10$), one from a non-normal distribution (left) and the other from a normal distribution (right). We take the average and \"studentize\" according to the formula for $T$ above {eq}`t-equation`. \n",
    "\n",
    "Look first at the upper left plot. The histogram of small-sample averages from the non-normal distribution do not fit well with a T-distribution on 9 degrees of freedom. However, the small-sample averages from the normal distribution in the upper right plot do!\n",
    "\n",
    "In the bottom row you can find histograms of simulated large sample averages ($n=50$), overlaid with the density function for a T-distribution on 49 degrees of freedom. The sample averages from both the non-normal (lower left) and normal (lower right) distributions end up fitting well with the T-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ad885ef",
   "metadata": {
    "scrolled": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "## Show four plots:\n",
    "## Top left: sampling distribution of small-sample averages from non-normal distribution\n",
    "## Top right: sampling distribution of small-sample averages from normal distribution\n",
    "## Bottom left: sampling distribution of large-sample averages from non-normal distribution\n",
    "## Bottom right: sampling distribution of large-sample averages from normal distribution\n",
    "\n",
    "def studentize(sample, pop_mean):\n",
    "    return ( sample.mean() - pop_mean ) / (sample.std() / np.sqrt(len(sample)))\n",
    "\n",
    "# top to bottom, left to right\n",
    "sample_means = pd.DataFrame(\n",
    "    np.array([\n",
    "        [studentize(scipy.stats.gamma.rvs(a=0.25, scale=4.0, size = 10), 1.0) for i in range(1,1000)],\n",
    "        [studentize(scipy.stats.norm.rvs(loc=1.0, scale=2.0, size = 10), 1.0) for i in range(1,1000)],\n",
    "        [studentize(scipy.stats.gamma.rvs(a=0.25, scale=4.0, size = 50), 1.0) for i in range(1,1000)],\n",
    "        [studentize(scipy.stats.norm.rvs(loc=1.0, scale=2.0, size = 50), 1.0) for i in range(1,1000)],\n",
    "    ])\n",
    ").T\n",
    "titles = [\n",
    "    [ \"small sample, non-normal\", \"small sample, normal\" ],\n",
    "    [ \"large sample, non-normal\", \"large sample, normal\"]\n",
    "]\n",
    "\n",
    "# use default seaborn palette\n",
    "palette = iter(sns.color_palette())\n",
    "histcolor = next(palette)\n",
    "tcolor = next(palette)\n",
    "\n",
    "fig, axs = plt.subplots(2,2, sharex=True, sharey=True, figsize=(10,6), layout='constrained')\n",
    "\n",
    "# plot histogram\n",
    "for i in range(0,2):\n",
    "    for j in range(0,2):\n",
    "        # get simulated data to plot\n",
    "        plot_data = sample_means[2*i+j]\n",
    "        # plot histogram\n",
    "        sns.histplot(\n",
    "            ax = axs[i,j],\n",
    "            data = plot_data,\n",
    "            stat = 'density',\n",
    "            color = histcolor,\n",
    "            linewidth = 0.3,\n",
    "            alpha = 0.6,\n",
    "            binwidth = 0.2,\n",
    "        )\n",
    "        # plot T-distribution\n",
    "        x = np.linspace(-5, 5, 100)\n",
    "        axs[i,j].plot(\n",
    "            x,\n",
    "            scipy.stats.t.pdf(x, df=len(plot_data) - 1),\n",
    "            color = tcolor, \n",
    "            linewidth = 4,\n",
    "            label = 'T-distribution' if i == 0 and j == 0 else '', # label only first t-distribution in legend\n",
    "        )\n",
    "        axs[i,j].set_xlim([-5,5])\n",
    "        axs[i,j].set_title(titles[i][j])\n",
    "        axs[i,j].set_xlabel('')\n",
    "fig.legend(loc='upper right')\n",
    "fig.suptitle(\"Sample Size and Normality for T-Distributions\", fontsize = 13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfe383",
   "metadata": {},
   "source": [
    "This shows us that, in order to use the T-test with a small sample, it's necessary to assume that our data comes from a normal distribution. But how we can tell if this assumption is true? This is a nuanced question. Because you don't have much data to start with, it's difficult to draw strong conclusions about the distribution it came from! Some textbooks will simply tell you to look for rules of thumb: as long as there are no outliers, or the graph looks roughly \"mound-shaped\", you're ok to proceed with the test. There are even hypothesis tests to quantify this or more sophisticated visuals to use, but they are outside of the scope of this chapter.\n",
    "\n",
    "### Robustness to Assumptions\n",
    "\n",
    "Statisticians have determined that the T-test is fairly \"robust\" to the normality assumptions. In simple terms, robustness means that you can still trust the results of the test, even if your assumptions are only approximately met. In this case, if your data comes from something that is close to normal, the T-test will still work properly. If that feels too imprecise, that's ok! It can take some time to develop your understanding and intuition for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48db1f",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0044166",
   "metadata": {},
   "source": [
    "The [Framingham Heart Study](https://en.wikipedia.org/wiki/Framingham_Heart_Study) is an observational study of cardiovascular health in the United States. The initial study followed over 5,000 volunteers from  Framingham, Massachusetts, USA for several decades, and followup studies even looked at their descendants. The study led to important findings in many areas, including a link between cholesterol and heart disease. (This exercise was inspired by an assignment from UC Berkeley's [Data 8](http://data8.org/).)\n",
    "\n",
    "We load in the data below. We find a number of different interesting variables to explore, but we will focus on cholesterol levels (`TOTCHOL`) versus the occurence of heart disease (`ANYCHD`). The variable `ANYCHD` takes the value `0` if the patient does not have heart disease and the value `1` if they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b911dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framingham = pd.read_csv(\"framingham.csv\")\n",
    "framingham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917c8b0",
   "metadata": {},
   "source": [
    "We want to determine if there is a difference in average cholesterol levels between people with heart disease and those without. \n",
    "\n",
    "1. State the hypotheses being tested.\n",
    "2. Describe the assumptions of the T-test and comment on if they are valid for this example.\n",
    "3. Compute the test statistic and $P$-value.\n",
    "4. What is your conclusion?\n",
    "5. Give a confidence interval for the difference in average total cholesterol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
