{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries and data import - needed for later code, will figure out to try and hide this later\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_dir = \"/Users/jonathanzhu/Documents/data/\"\n",
    "\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)\n",
    "text_df = text_df.query(\"sdg == 8 or sdg == 16 or sdg == 4\").copy()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Document Embedding</h1>\n",
    "\n",
    "<b>NOTE: The tensorflow package is very finicky and does not like working on Python 3.11.3 to our knowledge, or on computers that are not so powerful. However, it is the only available package (that we know of) for document embedding, and document embedding is nonetheless and important part of natural language processing. We plan on working around this problem for later editions of the book, but for now, any of the code can be skipped, and later sections will not depend on code from this one.</b>\n",
    "\n",
    "In the previous section, we represented variable length texts as fixed length numeric vectors; the approach we have used so far is the traditional approach of Bag of Words (BoW), which tokenizes a text into words (tokens), ignoring orders of tokens but may reserve the count. This approach is high dimension, and very sparse; this may result in over fitting and high time complexity.\n",
    "\n",
    "A more modern text vectorization approach is word embedding (also called simply embedding), relying on neural representations. This approach takes distributional semantics into account; that is, a word’s meaning is given by the words that frequently appear close-by. Hence, we can construct a word’s context by using the set of words that appear nearby within a fixed-sized window. \n",
    "\n",
    "Semantically similar texts, then, would appear closer to each other in the vector space. We could also possibly capture semantic operations by operations in the vector space; for example, similarity between texts could be measured by vector dot product. We could also perform algebraic operations; for example, \n",
    "\n",
    "$\\text{vector(”King”)} - \\text{vector(”Man”)} + \\text{vector(”Woman”)} \\sim \\text{vector(“Queen”)}$. \n",
    "\n",
    "Modern-day representations are typically learned from vast body of texts, often with deep neural networks, and they typically result in pre-trained models.\n",
    "\n",
    "To get embeddings, we use the $\\texttt{tensorflow}$ library, installed and imported as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 05:17:22.112447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. '/var/folders/px/b7vc3nh913zb_m0x36ncftj00000gn/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n\u001b[1;32m      8\u001b[0m embed_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m embed \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39;49mload(embed_url)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp2/lib/python3.9/site-packages/tensorflow_hub/module_v2.py:107\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    102\u001b[0m saved_model_pbtxt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    103\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(module_path),\n\u001b[1;32m    104\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(saved_model_path) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[0;32m--> 107\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m contains neither \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m nor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    109\u001b[0m                    (module_path, tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[1;32m    110\u001b[0m                     tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m options:\n\u001b[1;32m    113\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mgetattr\u001b[39m(tf, \u001b[39m\"\u001b[39m\u001b[39msaved_model\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39mLoadOptions\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. '/var/folders/px/b7vc3nh913zb_m0x36ncftj00000gn/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "#the below two lines should be run in the terminal to install\n",
    "#pip install tensorflow\n",
    "#pip install tensorflow_hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" # \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "embed = hub.load(embed_url) # print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to use the tokenizer found in the $\\texttt{nltk.data}$ library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonathanzhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we first need to break our document corpus into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_sentence = []\n",
    "text_df_sdg = []\n",
    "for (text, sdg) in iter(zip(text_df.text, text_df.sdg)):\n",
    "    sentence = sent_tokenize(text) \n",
    "    text_df_sentence = text_df_sentence + sentence\n",
    "    text_df_sdg = text_df_sdg + [sdg]*len(sentence)\n",
    "sentence_df = pd.DataFrame({\"text\": text_df_sentence, \"sdg\": text_df_sdg})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1</b>: What are the dimensions of the $\\texttt{text\\_df}$ and $\\texttt{sentence\\_df}$ dataframes? What do each of the dimension numbers represent?\n",
    "\n",
    "<b>Exercise 3.2</b>: Verify that the dimensions of the sentence dataframe are correct by breaking the original text dataframe into sentences and then determining the length.\n",
    "\n",
    "An important question to ask is how many sentences each document has; in other words, what is the distribution of the number of sentences in each text? We can determine that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     19649\n",
       "4     12018\n",
       "5      3723\n",
       "6      2601\n",
       "2       959\n",
       "7       622\n",
       "8       191\n",
       "1       154\n",
       "9        45\n",
       "10       22\n",
       "12       18\n",
       "13       10\n",
       "15       10\n",
       "11        8\n",
       "14        8\n",
       "16        5\n",
       "19        4\n",
       "17        3\n",
       "21        3\n",
       "20        2\n",
       "25        2\n",
       "18        1\n",
       "22        1\n",
       "24        1\n",
       "31        1\n",
       "40        1\n",
       "Name: num_sent, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"num_sent\"] = text_df.text.apply(lambda x: len(sent_tokenize(x)))\n",
    "text_df[\"num_sent\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the vast majority of the documents have less than 10 sentences, with most being only 3 or 4 sentences. This type of sentence embedding can help us with further NLP tasks down the line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.1 Universal Sentence Encoder</h2>\n",
    "\n",
    "The Universal Sentence Encoder (USE) was first published by Google around 2018. It maps a sentence, word, or short paragraph to a fixed length (typically 512) numeric vector. This approach would mean semantically similar sentences would be placed closer to each other in the embedding space. \n",
    "\n",
    "Embeddings are typically the result of using raw text, so no pre-processing would be involved. This sentence embedding can then be used for downstream applications,\n",
    "e.g., classification, clustering, and language prediction. \n",
    "\n",
    "USE is a pre-trained model trained on variety of data, e.g., wikipedia and books. It was trained with a deep averaging network (DAN) encoder; more information and explanation on the process behind USE can be found at https://arxiv.org/pdf/1803.11175.pdf.\n",
    "\n",
    "To utilize USE, we can take one of three approaches:\n",
    "<ol>\n",
    "<li>We could take our desired document, turn it into a collection of sentences, and then map each sentence to its respective vector;</li>\n",
    "<li>We could treat each document as a short paragraph and match each document to its respective vector, or;</li>\n",
    "<li>We could take a similar approach to #1, except then aggregate the vectors for each document to form a single vector per document.</li>\n",
    "</ol>\n",
    "\n",
    "To install USE, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time you run this, it may take some time (5+ minutes) to complete the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2 t-SNE</h2>\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding (t-SNE) is best used to scale text features to the same scale. In short, it is a method of dimension reduction (like PCA). t-SNE associates probabilities on a Student's t-distribution with each point; it then uses some randomization (hence Stochastic) to embed, paying particular attention to the neighbors of each point. While t-SNE will not be discussed further as to its specific mathematical methods, it can nonetheless be used for document embedding. More explanations on t-SNE can be found on the $\\texttt{scikit\\_learn}$ website, https://scikit-learn.org/stable/modules/manifold.html#t-sne. \n",
    "\n",
    "To demonstrate t-SNE, we will first start by running a classification algorithm called the Multilayer Perceptron on our UN SDG data \n",
    "\n",
    "When using t-SNE in Python, we can start with the following code. Note that t-SNE can take a long time to run; to help with this, the t-SNE documentation suggests using $\\texttt{MinMixScaler}$ so as to make everything the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 00:07:31.471784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string\n",
      "\t [[{{node inputs}}]]\n"
     ]
    }
   ],
   "source": [
    "docs = sentence_df.text\n",
    "categories = sentence_df.sdg\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(docs, categories, test_size=0.33, random_state=7)\n",
    "\n",
    "X_train_use_vector = embed(X_train.tolist())\n",
    "X_test_use_vector = embed(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3882    0.3780    0.3830      3249\n",
      "           2     0.4008    0.3763    0.3882      2899\n",
      "           3     0.5543    0.5545    0.5544      3118\n",
      "           4     0.5398    0.5539    0.5468      4432\n",
      "           5     0.5175    0.5159    0.5167      5119\n",
      "           6     0.4194    0.4277    0.4235      3283\n",
      "           7     0.4496    0.4652    0.4573      3710\n",
      "           8     0.1851    0.1860    0.1856      1747\n",
      "           9     0.3030    0.2583    0.2789      1827\n",
      "          10     0.2375    0.2569    0.2468      1857\n",
      "          11     0.3870    0.3606    0.3733      2718\n",
      "          12     0.2664    0.2869    0.2763      1328\n",
      "          13     0.3829    0.3981    0.3903      2517\n",
      "          14     0.4420    0.4012    0.4206      1386\n",
      "          15     0.3332    0.3847    0.3571      1861\n",
      "          16     0.7680    0.7593    0.7636      8783\n",
      "\n",
      "    accuracy                         0.4786     49834\n",
      "   macro avg     0.4109    0.4102    0.4102     49834\n",
      "weighted avg     0.4797    0.4786    0.4789     49834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_use_vector)\n",
    "X_train_use_vector_scaled = scaler.transform(X_train_use_vector)\n",
    "X_test_use_vector_scaled = scaler.transform(X_test_use_vector)\n",
    "\n",
    "use_mlp_clf = MLPClassifier(random_state=1, max_iter=500, hidden_layer_sizes=(300,)).fit(X_train_use_vector_scaled, y_train)\n",
    "y_pred = use_mlp_clf.predict(X_test_use_vector_scaled)\n",
    "print(metrics.classification_report(y_test,y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_use_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanifold\u001b[39;00m \u001b[39mimport\u001b[39;00m TSNE\n\u001b[0;32m----> 3\u001b[0m scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\u001b[39m.\u001b[39mfit(X_train_use_vector)\n\u001b[1;32m      4\u001b[0m X_train_use_vector_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_train_use_vector)\n\u001b[1;32m      5\u001b[0m X_test_use_vector_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test_use_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_use_vector' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_train_use_vector)\n",
    "X_train_use_vector_scaled = scaler.transform(X_train_use_vector)\n",
    "X_test_use_vector_scaled = scaler.transform(X_test_use_vector)\n",
    "\n",
    "tsne = TSNE(2, verbose=0, perplexity=50)\n",
    "tsne_proj = tsne.fit_transform(X_test_use_vector_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.3 More Exercises</h2>\n",
    "\n",
    "<b>Exercise 3.1</b>: Take two documents, one labeled as SDG 1 and the other as SDG 8. Segment these into sentences, compute the embedding, and find the dot product between the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
