{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries and data import - needed for later code, will figure out to try and hide this later\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_dir = \"/Users/jonathanzhu/Documents/data/\"\n",
    "\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Document Embedding\n",
    "\n",
    "<b>NOTE: The tensorflow package is very finicky and does not like working on Python 3.11.3 to our knowledge, or on computers that are not so powerful. However, it is the only available package (that we know of) for document embedding, and document embedding is nonetheless and important part of natural language processing. We plan on working around this problem for later editions of the book, but for now, any of the code can be skipped, and later sections will not depend on code from this one.</b>\n",
    "\n",
    "In the previous section, we represented variable length texts as fixed length numeric vectors; the approach we have used so far is the traditional approach of Bag of Words (BoW), which tokenizes a text into words (tokens), ignoring orders of tokens but may reserve the count. This approach is high dimension, and very sparse; this may result in over fitting and high time complexity.\n",
    "\n",
    "A more modern text vectorization approach is word embedding (also called simply embedding), relying on neural representations. This approach takes distributional semantics into account; that is, a word’s meaning is given by the words that frequently appear close-by. Hence, we can construct a word’s context by using the set of words that appear nearby within a fixed-sized window. \n",
    "\n",
    "Semantically similar texts, then, would appear closer to each other in the vector space. We could also possibly capture semantic operations by operations in the vector space; for example, similarity between texts could be measured by vector dot product. We could also perform algebraic operations; for example, \n",
    "\n",
    "$\\text{vector(”King”)} - \\text{vector(”Man”)} + \\text{vector(”Woman”)} \\sim \\text{vector(“Queen”)}$. \n",
    "\n",
    "Modern-day representations are typically learned from vast body of texts, often with deep neural networks, and they typically result in pre-trained models.\n",
    "\n",
    "To get embeddings, we use the $\\texttt{tensorflow}$ library, installed and imported as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:39:35.751494: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#the below two lines should be run in the terminal to install\n",
    "#pip install tensorflow\n",
    "#pip install tensorflow_hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#embed_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" # \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "#embed = hub.load(embed_url) # print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"/Users/jonathanzhu/Documents/bookchapter/source/universal-sentence-encoder_4\") # print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to use the tokenizer found in the $\\texttt{nltk.data}$ library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonathanzhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we first need to break our document corpus into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_sentence = []\n",
    "text_df_sdg = []\n",
    "for (text, sdg) in iter(zip(text_df.text, text_df.sdg)):\n",
    "    sentence = sent_tokenize(text) \n",
    "    text_df_sentence = text_df_sentence + sentence\n",
    "    text_df_sdg = text_df_sdg + [sdg]*len(sentence)\n",
    "sentence_df = pd.DataFrame({\"text\": text_df_sentence, \"sdg\": text_df_sdg})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3.1</b>: What are the dimensions of the $\\texttt{text\\_df}$ and $\\texttt{sentence\\_df}$ dataframes? What do each of the dimension numbers represent?\n",
    "\n",
    "<b>Exercise 3.2</b>: Verify that the dimensions of the sentence dataframe are correct by breaking the original text dataframe into sentences and then determining the length.\n",
    "\n",
    "An important question to ask is how many sentences each document has; in other words, what is the distribution of the number of sentences in each text? We can determine that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     19649\n",
       "4     12018\n",
       "5      3723\n",
       "6      2601\n",
       "2       959\n",
       "7       622\n",
       "8       191\n",
       "1       154\n",
       "9        45\n",
       "10       22\n",
       "12       18\n",
       "13       10\n",
       "15       10\n",
       "11        8\n",
       "14        8\n",
       "16        5\n",
       "19        4\n",
       "17        3\n",
       "21        3\n",
       "20        2\n",
       "25        2\n",
       "18        1\n",
       "22        1\n",
       "24        1\n",
       "31        1\n",
       "40        1\n",
       "Name: num_sent, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"num_sent\"] = text_df.text.apply(lambda x: len(sent_tokenize(x)))\n",
    "text_df[\"num_sent\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the vast majority of the documents have less than 10 sentences, with most being only 3 or 4 sentences. This type of sentence embedding can help us with further NLP tasks down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Universal Sentence Encoder\n",
    "\n",
    "The Universal Sentence Encoder (USE) was first published by Google around 2018. It maps a sentence, word, or short paragraph to a fixed length (typically 512) numeric vector. This approach would mean semantically similar sentences would be placed closer to each other in the embedding space. \n",
    "\n",
    "Embeddings are typically the result of using raw text, so no pre-processing would be involved. This sentence embedding can then be used for downstream applications,\n",
    "e.g., classification, clustering, and language prediction. \n",
    "\n",
    "USE is a pre-trained model trained on variety of data, e.g., wikipedia and books. It was trained with a deep averaging network (DAN) encoder; more information and explanation on the process behind USE can be found at https://arxiv.org/pdf/1803.11175.pdf.\n",
    "\n",
    "To utilize USE, we can take one of three approaches:\n",
    "<ol>\n",
    "<li>We could take our desired document, turn it into a collection of sentences, and then map each sentence to its respective vector;</li>\n",
    "<li>We could treat each document as a short paragraph and match each document to its respective vector, or;</li>\n",
    "<li>We could take a similar approach to #1, except then aggregate the vectors for each document to form a single vector per document.</li>\n",
    "</ol>\n",
    "\n",
    "To install USE, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(\"/Users/jonathanzhu/Documents/bookchapter/source/universal-sentence-encoder_4\")\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time you run this, it may take some time (5+ minutes) to complete the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 More Exercises\n",
    "\n",
    "<b>Exercise 3.1</b>: Take two documents, one labeled as SDG 1 and the other as SDG 8. Segment these into sentences, compute the embedding, and find the dot product between the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
