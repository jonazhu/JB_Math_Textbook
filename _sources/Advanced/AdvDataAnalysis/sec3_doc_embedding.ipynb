{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Document Embedding\n",
    "\n",
    "In the previous section, we represented variable length texts as fixed length numeric vectors; the approach we have used so far is the traditional approach of Bag of Words (BoW), which tokenizes a text into words (tokens), ignoring orders of tokens but may reserve the count. This approach is high dimension, and very sparse; this may result in over fitting and high time complexity.\n",
    "\n",
    "A more modern text vectorization approach is word embedding (also called simply embedding), relying on neural representations. This approach takes distributional semantics into account; that is, a word’s meaning is given by the words that frequently appear close-by. Hence, we can construct a word’s context by using the set of words that appear nearby within a fixed-sized window. \n",
    "\n",
    "Semantically similar texts, then, would appear closer to each other in the vector space. We could also possibly capture semantic operations by operations in the vector space; for example, similarity between texts could be measured by vector dot product. We could also perform algebraic operations; for example, \n",
    "\n",
    "$\\text{vector(”King”)} - \\text{vector(”Man”)} + \\text{vector(”Woman”)} \\sim \\text{vector(“Queen”)}$. \n",
    "\n",
    "Modern-day representations are typically learned from vast body of texts, often with deep neural networks, and they typically result in pre-trained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Universal Sentence Encoder\n",
    "\n",
    "The Universal Sentence Encoder (USE) was first published by Google around 2018. It maps a sentence, word, or short paragraph to a fixed length (typically 512) numeric vector. This approach would mean semantically similar sentences would be placed closer to each other in the embedding space. \n",
    "\n",
    "Embeddings are typically the result of using raw text, so no pre-processing would be involved. This sentence embedding can then be used for downstream applications,\n",
    "e.g., classification, clustering, and language prediction. \n",
    "\n",
    "USE is a pre-trained model trained on variety of data, e.g., wikipedia and books. It was trained with a deep averaging network (DAN) encoder; more information and explanation on the process behind USE can be found at https://arxiv.org/pdf/1803.11175.pdf.\n",
    "\n",
    "To utilize USE, we can take one of three approaches:\n",
    "<ol>\n",
    "<li>We could take our desired document, turn it into a collection of sentences, and then map each sentence to its respective vector;</li>\n",
    "<li>We could treat each document as a short paragraph and match each document to its respective vector, or;</li>\n",
    "<li>We could take a similar approach to #1, except then aggregate the vectors for each document to form a single vector per document.</li>\n",
    "</ol>\n",
    "\n",
    "To install USE, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "pip install tensorflow_hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "return model(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time you run this, it may take some time (5+ minutes) to complete the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 t-SNE\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding (t-SNE) is best used to scale text features to the same scale. In short, it is a method of dimension reduction (like PCA). t-SNE associates probabilities on a Student's t-distribution with each point; it then uses some randomization (hence Stochastic) to embed, paying particular attention to the neighbors of each point. \n",
    "\n",
    "While t-SNE will not be discussed further as to its specific methods, it can nonetheless be used for document embedding. More explanations on t-SNE can be found on the $\\texttt{scikit\\_learn}$ website, https://scikit-learn.org/stable/modules/manifold.html#t-sne. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 More Exercises\n",
    "\n",
    "<b>Exercise 3.1</b>: Take two documents, one labeled as SDG 1 and the other as SDG 8. Segment these into sentences, compute the embedding, and find the dot product between the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
