{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Evaluating Classification</h1>\n",
    "\n",
    "In many classification problems, the metrics we use include precision, recall. f1, accuracy, and average accuracy (both macro and micro). Additional common metrics for classification include ROC (Receiver Operating Characteristics) and AUC (Area Under the Curve), often combined as ROC-AUC, and for multi-class classification, we can test the model's ability to classify items on a One vs. Rest (OvR) or One vs. One (OvO) basis. \n",
    "\n",
    "In Section 2, we defined precision, recall, and f1. Accuracy is self-explanatory as the proportion of correctly predicted classes as a whole. The micro average accuracy calculates the average accuracy per class, while macro average accuracy similarly calculates this but takes class imbalance into account. \n",
    "\n",
    "For this section, we're going to focus on the results from our ridge classification in the previous section. Notice that we get the specific results for each goal on its own, in addition to a confusion matrix for the specific results on the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_df.embedding.tolist()\n",
    "scaler = preprocessing.MinMaxScaler().fit(docs)\n",
    "X = scaler.transform(docs)\n",
    "y = text_df.sdg\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "ridge_clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "ridge_clf = ridge_clf.fit(X_train, y_train)\n",
    "y_pred = ridge_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test,y_pred, digits = 4))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.1 The ROC Curve</h2>\n",
    "\n",
    "The goal of any classifier is to maximize true positive rate (TPR) while minimizing false positive rate (FPR). The ROC curve is then determined as follows: for any given threshold, we line up the points, with TPR on the Y-axis and FPR on the X-axis. Points at the top left corner of the plot imply an FPR of 0 and a TPR of 1 (i.e., perfect classification). \n",
    "\n",
    "A model can be evaluating qualitatively by judging the “steepness” of its ROC curve; this evaluation can be made quantitative by finding the Area Under the Curve (AUC). A larger AUC implies a steeper curve and is <i>usually</i> better.\n",
    "\n",
    "For binary classification, each point on the ROC curve represents a different threshold and can be a choice for the final classifier to be used; the choice made typically depends on the business requirements and constraints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.2 Multiclass Classifiers</h2>\n",
    "\n",
    "In any multiclass classification problem, there are multiple class labels $(c_1, c_2, …, c_k)$ in the data, and each training sample is labeled as belonging to one class only. The strategies used with binary classifiers can be taken in multiclass classifiers by reducing the problem to binary classification. There are two strategies typically taken to do this, those being One vs. Rest (OvR) and One vs. One (OvO).\n",
    "\n",
    "<h3>5.2.1 One vs Rest</h3>\n",
    "The OvR method of evaluation judges the model ability to determine if a given item belongs to a given class or if it is in one of the other classes. For each class label $c$ in $(c_1, c_2, …, c_k)$, we say that the <i>positive samples</i> are those that are labeled as class $c$, and the <i>negative samples</i> are all the rest of the samples. We then fit and predict for class $c$.\n",
    "\n",
    "For scoring each sample, we simply take the highest probable score (i.e., highest score) of the $k$ classifiers as the class for the sample. \n",
    "\n",
    "When looking at the ROC curve for the OvR strategy on our classification, we get the following graphic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_df.embedding.tolist()\n",
    "scaler = preprocessing.MinMaxScaler().fit(docs)\n",
    "X = scaler.transform(docs)\n",
    "y = text_df.sdg\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y)\n",
    "y_onehot = label_binarizer.transform(y)\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "class_names = [sdg_names[sdg_names[\"sdg\"] == label_binarizer.classes_[i]].sdg_name.item() \\\n",
    "               for i in range(n_classes)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=.33, random_state=0)\n",
    "ovr_mlp_clf = OneVsRestClassifier(MLPClassifier(random_state=0, max_iter=300)).fit(X_train,y_train)\n",
    "y_score = ovr_mlp_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for class_id in range(n_classes):\n",
    "    fpr[class_id], tpr[class_id], _ = metrics.roc_curve(y_test[:, class_id], y_score[:, class_id]) # roc_curve works on binary\n",
    "    roc_auc[class_id] = metrics.auc(fpr[class_id], tpr[class_id])\n",
    "\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[class_id], tpr[class_id], color=color, lw=1.5,alpha = 1, \n",
    "             label='SDG {0} - {1} (AUC = {2:0.2f})'\n",
    "             ''.format(class_id+1, class_names[class_id], roc_auc[class_id]))\n",
    "plt.plot([0, 1], [0, 1], '--', lw=1, color=\"lightgrey\")\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC for SDG multiclass classifier')\n",
    "plt.legend(loc=\"lower right\", fontsize=6)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of the goals, aside from goal 17, gets its own curve, and our program is able to report the AUC score for each one. \n",
    "\n",
    "<b>Exercise 5.1</b>: Create a similar ROC curve plot for your Naive Bayes classification and Multilayer Perceptron classification from the previous section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2.2 One vs One</h3>\n",
    "\n",
    "The OvO approach compares each class to another single class for all possible pairs of classes. For each class label $c$ in $(c_1, c_2, …, c_k)$, and for each pair $(c, c_j)$, with $c_j \\neq c$, we say that the <i>positive samples</i> are those that are labeled as class $c$, and the <i>negative samples</i> are those that are labeled as class $c_j$.\n",
    "\n",
    "For each sample to be scored, it gets $k-1$ classification results, and we then vote by majority to determine the class for the sample. \n",
    "\n",
    "Similar to the OvR strategy, we can produce an ROC curve for the OvO strategy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "class_of_interest = 8 # SDG 8\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test[:, class_id],\n",
    "    y_score[:, class_id],\n",
    "    name=f\"SDG {class_of_interest} vs the rest\",\n",
    "    color=\"darkorange\",\n",
    "    ax = ax,\n",
    ")\n",
    "\n",
    "class_of_interest = 16\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test[:, class_id],\n",
    "    y_score[:, class_id],\n",
    "    name=f\"SDG {class_of_interest} vs the rest\",\n",
    "    color=\"purple\",\n",
    "    ax = ax,\n",
    ")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"--\", label=\"chance level (AUC = 0.5)\", color = \"grey\")\n",
    "plt.plot([0.05, 0.05], [0, 1], \"--\", label=\"FPT at 0.05 level\", color = \"green\")\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we identified two classes that which we wanted to specifically examine, and these two are the only curves that are present on the plot. Note that the AUC for these goals we picked are different from those in the OvR strategy. \n",
    "\n",
    "<b>Exercise 5.2</b>: Create another ROC curve plot for three additional goals of your choosing.\n",
    "\n",
    "<b>Exercise 5.3</b>: Using SDGs 8 and 16, recreate the ROC curve plot using your Naive Bayes and Multilayer Perceptron classifications from the previous sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.3 More Exercises</h2>\n",
    "\n",
    "<b>Exercise 5.4</b>: Modify your function from Exercise 3.1 to now output the test set metrics for the classification you run. Be sure to include precision, recall, f1, and accuracy. \n",
    "\n",
    "<b>Exercise 5.5</b>: What does a point on the ROC curve at (1,1) indicate about the corresponding model? Similarly, what does a point at (0,0) on the curve indicate about the model?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
