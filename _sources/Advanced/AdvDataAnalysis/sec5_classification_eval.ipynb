{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Evaluating Classification</h1>\n",
    "\n",
    "In many classification problems, the metrics we use include precision, recall. f1, accuracy, and average accuracy (both macro and micro). Additional common metrics for classification include ROC (Receiver Operating Characteristics) and AUC (Area Under the Curve), often combined as ROC-AUC, and for multi-class classification, we can test the model's ability to classify items on a One vs. Rest (OvR) or One vs. One (OvO) basis. \n",
    "\n",
    "In Section 2, we defined precision, recall, and f1. Accuracy is self-explanatory as the proportion of correctly predicted classes as a whole. The micro average accuracy calculates the average accuracy per class, while macro average accuracy similarly calculates this but takes class imbalance into account. \n",
    "\n",
    "<h2>5.1 The ROC Curve</h2>\n",
    "\n",
    "The goal of any classifier is to maximize true positive rate (TPR) while minimizing false positive rate (FPR). The ROC curve is then determined as follows: for any given threshold, we line up the points, with TPR on the Y-axis and FPR on the X-axis. Points at the top left corner of the plot imply an FPR of 0 and a TPR of 1 (i.e., perfect classification). \n",
    "\n",
    "A model can be evaluating qualitatively by judging the “steepness” of its ROC curve; this evaluation can be made quantitative by finding the Area Under the Curve (AUC). A larger AUC implies a steeper curve and is <i>usually</i> better.\n",
    "\n",
    "For binary classification, each point on the ROC curve represents a different threshold and can be a choice for the final classifier to be used; the choice made typically depends on the business requirements and constraints.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.2 Multiclass Classifiers</h2>\n",
    "\n",
    "In any multiclass classification problem, there are multiple class labels $(c_1, c_2, …, c_k)$ in the data, and each training sample is labeled as belonging to one class only. The strategies used with binary classifiers can be taken in multiclass classifiers by reducing the problem to binary classification. There are two strategies typically taken to do this, those being One vs. Rest (OvR) and One vs. One (OvO).\n",
    "\n",
    "<h3>5.2.1 One vs Rest</h3>\n",
    "The OvR method of evaluation judges the model ability to determine if a given item belongs to a given class or if it is in one of the other classes. For each class label $c$ in $(c_1, c_2, …, c_k)$, we say that the <i>positive samples</i> are those that are labeled as class $c$, and the <i>negative samples</i> are all the rest of the samples. We then fit and predict for class $c$.\n",
    "\n",
    "For scoring each sample, we simply take the highest probable score (i.e., highest score) of the $k$ classifiers as the class for the sample. \n",
    "\n",
    "<h3>5.2.2 One vs One</h3>\n",
    "\n",
    "The OvO approach compares each class to another single class for all possible pairs of classes. For each class label $c$ in $(c_1, c_2, …, c_k)$, and for each pair $(c, c_j)$, with $c_j \\neq c$, we say that the <i>positive samples</i> are those that are labeled as class $c$, and the <i>negative samples</i> are those that are labeled as class $c_j$.\n",
    "\n",
    "For each sample to be scored, it gets $k-1$ classification results, and we then vote by majority to determine the class for the sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.3 More Exercises</h2>\n",
    "\n",
    "<b>Exercise 5.1</b>: Modify your function from Exercise 3.1 to now output the test set metrics for the classification you run. Be sure to include precision, recall, f1, and accuracy. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
