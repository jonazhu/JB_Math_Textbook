{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jonathanzhu/Documents/data/osdg-community-data-v2023-01-01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/jonathanzhu/Documents/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m text_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mosdg-community-data-v2023-01-01.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m text_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_dir \u001b[38;5;241m+\u001b[39m text_file_name,sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m col_names \u001b[38;5;241m=\u001b[39m text_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m text_df[col_names] \u001b[38;5;241m=\u001b[39m text_df[text_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jonathanzhu/Documents/data/osdg-community-data-v2023-01-01.csv'"
     ]
    }
   ],
   "source": [
    "#libraries and data import - needed for later code, will figure out to try and hide this later\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_dir = \"/Users/jonathanzhu/Documents/data/\"\n",
    "\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} Natural language processors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, natural language processors (NLP) refer to both speech data and textual data. There are several components necessary to understand the structure and meaning of human language.\n",
    "\n",
    "From a linguistics perspective, it is important to look at the following:\n",
    " - syntax, the actual rules that define how words are combined to form understandable sentences;\n",
    " - semantics, referring to the meaning behind the phrases and sentences formed by syntax;\n",
    " - morphology, referring to the ways in which words can take different forms.\n",
    "\n",
    "Then, from a computational perspective, we can take the rules from the linguistics aspects and transform linguistic knowledge into rule-based and/or ML-based algorithms to solve problems related to natural language processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercises\n",
    "<b>Exercise 1.1:</b> Explore the Allen NLP demos found at https://demo.allennlp.org/sentiment-analysis/glove-sentiment-analysis. \n",
    "<ol>\n",
    "<li>Choose either of the two sentiment analysis algorithms and use any of the provided example sentences. Look under CLI Output. How do the algorithms take the example input sentence and transform it?</li>\n",
    "<li> Look under Model Interpretations and Model Attacks, which visualize the most important words in the sentence. Which words seem to be important, and which ones are often removed? </li>\n",
    "<li>Try some of your own sentences. Does the algorithm seem to be accurate?</li>\n",
    "</ol>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tasks\n",
    "Within the realm of preprocessing textual data, we first must perform certain detections and tasks; this ensures that we have textual data that makes sense and is meaningful. The following all help at ensuring data cleanliness:\n",
    "\n",
    " - <b>Syntactic analysis:</b> Does the grammar make sense? Is the text gramatically correct?\n",
    " - <b>Semantic analysis:</b> Are we aware of the meaning of the words in context?  Do we understand the structure, word interaction, and related concepts of the text?\n",
    " - <b>Keyword extraction:</b> What are the most important words in the text?\n",
    " - <b>Named entity resolution:</b> How do we identify and extract entities (names, organizations, addresses and places) and relationships? \n",
    " - <b>Text classification:</b> Can we organize our text into predefined categories? Ways of doing this include:\n",
    "   - <b>Sentiment analysis:</b> Can we classify text (like customer feedback) into positive and negative feedback?\n",
    "   - <b>Email filtering:</b> If we are taking our texts from emails, can we classify email text as spam mail and remove spam?\n",
    "   - <b>Intent detection:</b> What is the text generator trying to achieve? For example, searching “apple” could indicate an intent of buying, eating, or researching apples.\n",
    "   - <b>Language detection:</b> Can we classify the body of text into languages, with an associated probability?\n",
    "\n",
    "<b>Exercise 1.2:</b> Explore another Allen NLP demo found at https://demo.allennlp.org/next-token-lm. \n",
    "<ol>\n",
    "<li>Try some of the provided example sentences, and give the probabilities for the sentences with the highest score. </li>\n",
    "</ol>\n",
    "\n",
    "Looking at the UN SDG dataset, we find that it includes positive or negative labels. Since each text is classified with a single goal, positive labels indicate the agreement of that text with the goal, while negative labels indicate disagreement. We use \\texttt{sum()}$ to sum the negative labels and the positive labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.labels_negative.sum() + text_df.labels_positive.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the sum of the positively and negatively labeled texts does not equal the full amount of texts imported from the dataset. Hence, this provides some beginning rationale for the necessity of preprocessing.\n",
    "\n",
    "Data preprocessing for textual data includes some specific tasks, but it also includes some work that can be done on datasets in general. For our UN SDG dataset, it is useful to remove the columns that we don't need, which for this dataset are the columns with zeroes in all entries. We do this by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_df\u001b[38;5;241m.\u001b[39mdrop(text_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_df' is not defined"
     ]
    }
   ],
   "source": [
    "text_df.drop(text_df.columns.values[0],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a good idea to check our data for any discrepancies, especially N/A values and incorrect entries. \n",
    "\n",
    "<b>Exercise 1.3</b>: Check the data for any N/A values. Is it good to remove those entries with N/A values?\n",
    "\n",
    "For our UN SDG dataset, we want texts that are clearly classified into a single goal. We also will want to select for only the positively labeled texts as this indicates higher agreement with the labeled goal, so our filtering will remove the rows with labeling agreement less than 0.5 and the rows where the positive labels minus the negative labels is less than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_df \u001b[38;5;241m=\u001b[39m text_df\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magreement > 0.5 and (labels_positive - labels_negative) > 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m text_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_df' is not defined"
     ]
    }
   ],
   "source": [
    "text_df = text_df.query(\"agreement > 0.5 and (labels_positive - labels_negative) > 2\")\n",
    "text_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.4</b>: Use $\\texttt{text\\_df.info()}$ to examine information about the dataset after running the preprocessing in this section. What columns are included? What data types are there, and how many entries? Does this seem like a reasonable size of data to work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Levels of Processing\n",
    "\n",
    "In (almost) any language, there are distinct levels of a text we can focus on: <b>characters</b> are strung together to form <b>words</b>, which are put together into <b>sentences</b>, then <b>paragraphs</b>, <b>documents</b>, and a whole set of documents, or a <b>corpus</b>. In all these cases, we put multiples together to make the next level, and decisions in preprocessing at one level will inherently impact the next level.\n",
    "\n",
    "We will not be focusing on using algorithms to process whole documents or anything above that level, so we can look at preprocessing at the remaining levels: characters, words, sentences, and paragraphs. \n",
    "\n",
    "<b>For characters</b>, we typically process textual data by removing special characters, punctuations, and normalizations. However, this is not necessarily the case for every NLP approach. In some cases, taking away these special characters, punctuations, and normalizations will completely alter the meaning of higher levels of text.\n",
    "\n",
    "<b>For words</b>, we first have to define words within our text snippets. This is known as segmenting: in the English language, we can use white space to segment text into words. However, this is not always the case with other languages, so it is important to know the linguistic rules of the primary language you are working with. Additionally, we perform some removals and/or replacements for odd words such as abbreviations, acronyms, numbers, and misspelled words. Finally, we normalize our text data; this is typically done by stemming (covered later in this section) or lemmatization.\n",
    "\n",
    "<b>For sentences</b>, we first have to define sentence boundaries. In most languages, including English, sentences are started with a capital letter and ended with a period. However, periods and capital letters are also used within sentences, and captial letters or periods might not even exist in other languages, again stressing the importance of knowing the language's rules.  Within sentences, we can also mark phrases. In English sentences, we can typically label phrases of sentences with a subject, predicate, and object; the subject is the one performing the action, or predicate, on or directed towards the object. Finally, we parse the sentence, or tag the words with their respective part of speech. Note that this can only be done on the sentence level, not the word level, as the same word can have a different part of speech depending on how it is used in a sentence.\n",
    "\n",
    "Finally, <b>for paragraphs</b>, our primary level of preprocessing is to understand the text by extracting the meaning of the text. This can include sentiments, emotions, intentions, etc. A good way of doing this is to perform abstractive summarization, or constructing new text that is similar in meaning to the old text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual process of text preprocessing, however, does not necessarily flow distinctly from one level to another. The order of preprocessing steps, as well as whether or not to include specific steps, depends largely on the application. An example process is given here: \n",
    "<ol>\n",
    "<li>Segmentation: breaking text into sentences</li>\n",
    "<li>Spelling correction</li>\n",
    "<li>Noise removal: includes text that would otherwise confuse the main text, including emojis, foreign language, and http links</li>\n",
    "<li>Language detection: identifying the language used in a body of text</li>\n",
    "<li>Stop-words removal: these words are typically high frequency, generic, and less context specific\n",
    "change cases</li>\n",
    "<li>Lemmatization</li>\n",
    "<li>Tokenization: break texts into words, phrases, symbols, and other semantically useful units, meaningful elements</li>\n",
    "<li>Parsing: part of speech identification</li>\n",
    "<li>Standardization</li>\n",
    "<li>Stemming: reduces tokens to base forms (stems), removes affixes</li>\n",
    "</ol>\n",
    "\n",
    "Many of these steps may be unfamiliar - that is perfectly fine, as they will be covered later in this section. \n",
    "\n",
    "It is commonly accepted that preprocessing text helps improve accuracy, so it is critical to perform good, appropriate preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Procedures\n",
    "\n",
    "### 1.3.1 Stemming \n",
    "\n",
    "Developed by Martin Porter in 1980, the <b>Porter Stemmer</b> is the oldest and most commonly used stemming algorithm in many languages. It is slightly time consuming, so another stemming algorithm that can be used is the Lancaster stemmer. This stemmer is very aggressive, however, and is especially harmful for short tokens, where the token itself may become unclear or altered to a point where it loses meaning.\n",
    "\n",
    "As mentioned previously, stemming is performed to help normalize our textual data; this is done so that we do not end up with many different words with very similar meaning. For example, stemming turns \"watch\", \"watched\", \"watching\", and \"watches\" into \"watch\". To do this, the algorithm accesses a database where known word forms, such as the various forms of \"watch\", are grouped together, then analyzes the text for all instances of these forms and turns them all into the root word. Stemmers also typically perform preprocessing tasks like turning words not at the beginning of a sentence into lowercase and removing stop words, which are common words like \"the\" and \"that\" which show up in a variety of contexts and typically do not change meaning across these contexts.\n",
    "\n",
    "However, there are always irregularities when using any stemming algorithm. Take the following examples: \n",
    "<ul>\n",
    "<li>\"I bought an Apple\" vs. \"I bought an apple\": Here, the stemming algorithm removed the capital on \"Apple\", but we can clearly see that this drastically altered the meaning; now, the speaker has bought a fruit instead of a product from the company Apple.  </li>\n",
    "<li>\"I watched <i>The Who</i>\" => remove stop words => \"I watched\": Here, the stemming algorithm removed the stop words \"the\" and \"who\", but this again changed the meaning of the sentence. Here, instead of watching the TV show <i>The Who</i>, the speaker has now watched something unspecified.</li> \n",
    "<li>\"Take that!\" => remove stop words => \"Take\": Like the above example, the stemming algorithm removed a stop word, \"that\". However, the sentence now loses all meaning: it is no longer a complete sentence, and without the \"that\" which served to add a violent, fighting intent to the original sentence, this emotion and intent is completely gone.\n",
    "</ul>\n",
    "\n",
    "Let's try these stemmers, as well as a third type, the Snowball Stemmer, on some tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['narr', 'comprehens', 'publicis', 'moder', 'prosper', 'legitim', 'accord', 'glorifi']\n",
      "Snowball Stemmer: ['narrat', 'comprehens', 'publicis', 'moder', 'prosper', 'legitim', 'accord', 'glorifi']\n",
      "Lancaster Stemmer: ['nar', 'comprehend', 'publ', 'mod', 'prosp', 'legitim', 'accord', 'glor']\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "\n",
    "tokens =  ['narrative', 'comprehensives', 'publicised', 'moderately', 'prosperous', 'legitimizes', 'according', 'glorifies'] \n",
    "\n",
    "porter = stem.porter.PorterStemmer() # Porter stemmer\n",
    "snowball = stem.snowball.EnglishStemmer() # Snowball stemmer\n",
    "lancaster = stem.lancaster.LancasterStemmer() # Lancaster stemmer\n",
    "\n",
    "print('Porter Stemmer:', [porter.stem(i) for i in tokens])\n",
    "print('Snowball Stemmer:', [snowball.stem(i) for i in tokens])\n",
    "print('Lancaster Stemmer:', [lancaster.stem(i) for i in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1.5</b>: How do the different stemmers compare for the list of words given to it in the above example? Which one might have the most trouble on a larger set of tokens>\n",
    "\n",
    "<b>Exercise 1.6</b>: Modify the above code to stem your own tokens. What words seem to be removed as stop words? How are words commonly shortened, and what types of words seem to cause the most trouble for the stemmers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Tokenizers\n",
    "\n",
    "The primary goal of a tokenizer is to break text into tokens. This is highly language dependent; in English, we use white space and punctuations to as word and token separators, but this may not work in some languages. \n",
    "\n",
    "For the purposes of building ML models, there are several things that tokenizers track in addition to creating the tokens themselves. Tokenizers track <i>features</i>, or the frequency of occurrence of each individual token (normalized or not). They also produce a <i>sample</i>, or a vector of all the token frequencies for a given document. \n",
    "\n",
    "When a tokenizer vectorizes a corpus, it counts the occurrences of tokens in each document, normalizes and weights these occurrences, and constructs an n-gram, which is discussed in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Named Entity Recognition\n",
    "\n",
    "The purpose of named entity recognition (NER) is self-explanatory; it extracts entities from data.\n",
    "\n",
    "<b>Exercise 1.10:</b> Try the Allen NLP NER demo found at https://demo.allennlp.org/named-entity-recognition/named-entity-recognition. \n",
    "<ol>\n",
    "<li>Use the below sentence in the demo. Which phrases are labeled as named entities? </li>\n",
    "\n",
    "'The Israel Oceanographic and Limnological Research station monitors the quantity and quality of water along the coastline of the Mediterranean Sea. The Nature and Parks Authority (NPA) monitors water quality in rivers on behalf of the MoEP. Mekorot and local authorities monitor drinking water quality under the supervision of the Ministry of Health. The Ministry of Health monitors effluent quality prior to its use in the agricultural sector.'\n",
    "\n",
    "<li>Now use text.lower() to make the entire sentence lowercase and use that as an input into the Allen NLP NER. Which phrases are now labeled as named entities?</li>\n",
    "\n",
    "'the israel oceanographic and limnological research station monitors the quantity and quality of water along the coastline of the mediterranean sea. the nature and parks authority (npa) monitors water quality in rivers on behalf of the moep. mekorot and local authorities monitor drinking water quality under the supervision of the ministry of health. the ministry of health monitors effluent quality prior to its use in the agricultural sector.'\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 String Operations\n",
    "\n",
    "Now that we know about various preprocessing techniques, we can review some basic string operations. These will also be important in identifying various sentences and tokens within our dataset in case we would like to focus on specific words, or it can help in identifying cases, numeric characters, and other tests that we might need to conduct. A few are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s\u001b[38;5;241m.\u001b[39mstartswith(t) \u001b[38;5;66;03m#test if s starts with t\u001b[39;00m\n\u001b[1;32m      2\u001b[0m s\u001b[38;5;241m.\u001b[39mendswith(t) \u001b[38;5;66;03m#test if s ends with t\u001b[39;00m\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;129;01min\u001b[39;00m s \u001b[38;5;66;03m# test if t is a substring of s\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s.startswith(t) #test if s starts with t\n",
    "s.endswith(t) #test if s ends with t\n",
    "t in s # test if t is a substring of s\n",
    "s.islower() # test if s contains cased characters and all are lowercase\n",
    "s.isupper() # test if s contains cased characters and all are uppercase\n",
    "s.isalpha() # test if s is non-empty and all characters in s are alphabetic\n",
    "s.isalnum() # test if s is non-empty and all characters in s are alphanumeric\n",
    "s.isdigit() # test if s is non-empty and all characters in s are digits\n",
    "s.istitle() # test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example to work through, let's take the Chinese phrase '四个全面' and see if we can locate it in the texts. This phrase, anglicized as \"sigequanmian\" and roughly translating to \"four comprehensives\", was publicized by Chinese President Xi Jinping as a set of goals for China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_df[text_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m四个全面\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_df' is not defined"
     ]
    }
   ],
   "source": [
    "text_df[text_df[\"text\"].apply(lambda x: x.find('四个全面')) > 0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible string operation is comparison, as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t1 \u001b[38;5;241m=\u001b[39m text_df[text_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m四个全面\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(t1 \u001b[38;5;241m==\u001b[39m text_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m18009\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(t1 \u001b[38;5;129;01mis\u001b[39;00m text_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m18009\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_df' is not defined"
     ]
    }
   ],
   "source": [
    "t1 = text_df[text_df[\"text\"].apply(lambda x: x.find('四个全面')) > 0][\"text\"].values[0]\n",
    "print(t1 == text_df.iloc[18009].text)\n",
    "print(t1 is text_df.iloc[18009].text)\n",
    "print(t1 == text_df.iloc[18009].text.casefold())\n",
    "print(t1.lower() == text_df.iloc[18009].text.casefold())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Python takes uses strict comparisons, meaning that the two sentences will not be equal unless the cases (capital and lowercase) are matched.\n",
    "\n",
    "Note that in the above example, the $\\texttt{.lower()}$ and $\\texttt{.casefold()}$ yielded equal results. However, this is not the case for all languages. Take the German letter ß:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using casefold(): gross\n",
      "Using lower(): groß\n"
     ]
    }
   ],
   "source": [
    "text = 'groß'\n",
    "\n",
    "# convert text to lowercase using casefold()\n",
    "print('Using casefold():', text.casefold()) \n",
    "\n",
    "# convert text to lowercase using lower()\n",
    "print('Using lower():', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German word \"groß\" translates as large or big, and this meaning is kept using $\\texttt{.lower()}$. However, when using $\\texttt{.casefold()}$, it becomes \"gross\", which is used in English to indicate disgust. It is up to you to decide which of these to use when converting to lowercase; however, it is best to keep this consistent across all your code so as to avoid potential problems such as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 More Exercises"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
