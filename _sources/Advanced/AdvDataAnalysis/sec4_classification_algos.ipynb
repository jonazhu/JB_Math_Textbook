{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification of Texts\n",
    "\n",
    "Now that we have gone through the arduous process of preprocessing, transforming, and embedding our text data, we can finally get into some of the was we can work with it, most prominent of which is classification. In the case of our UN SDG dataset, our goal is to classify documents by the goal they match with. In this subsection, we will briefly review the general process of machine learning before going into some of the methods by which text data are commonly classified.\n",
    "\n",
    "## 4.1 A Machine Learning Overview\n",
    "\n",
    "Machine learning of any kind can be boiled down to the process by which we train a machine to make the most effective predictions. When we train a machine to make numerical predictions, we perform <b>regression</b>; if we train a machine to categorize data into categories, we perform <b>classification</b>. Since our UN SDG dataset is classified into the categories of the goals, we will focus on classification.\n",
    "\n",
    "There are two main types of machine learning: <b>supervised</b> and <b>unsupervised</b>. Supervised learning means that the data we give the machine is already labeled with the \"correct\" value or category, while unsupervised learning means that the data is unlabeled and there is no \"correct\" value or category. Here, since our UN SDG data is already labeled with the goal it aligns most with, we consider it supervised. As such, the goal for the machine is to match the \"correct\" labels of the data we give it.\n",
    "\n",
    "Machine learning starts with a dataset; with modern advances in data collection and storage, machine learning datasets typically involve thousands of data points, while some industry machine learning projects have access to more powerful computers and can use datasets with millions of data points. This dataset is split into a <b>training set</b> and a <b>testing set</b>. The training set consists of the data fed to the machine for it to build its model, while the testing set consists of data kept separate to evaluate the model's performance. Typically, the training set consists of more data than the testing set; it is common to split the main set 70/30 (70% training, 30% testing) or 80/20.\n",
    "\n",
    "When a machine builds it predictive model, we need to understand that no machine is perfect at predicting (though some can get pretty close). As such, the main goal of an algorithm when given the training set is to <b>minimize error</b>. The process of error minimization varies between algorithms; one common process is gradient descent, which aims to find a minimum for the error function (a function which determines the error based on the function's internal parameters). \n",
    "\n",
    "However, to truly judge the efficacy of the algorithm, we must use data that the algorithm has not yet seen. Hence, the testing set is used to evaluate the algorithm's performance on data similar (but different) from the training set. As data scientists, our goal would be to maximize the performance on the testing set, and we do so by several means discussed in section 5.\n",
    "\n",
    "For working with all the algorithms, we will use the same train/test split of 66/33:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_df.text\n",
    "categories = text_df.sdg\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(docs, categories, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then vectorize using TF-IDF from the previous section; for this, we are using bi-grams. Unigrams will be given as exercises throughout the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words = \"english\", min_df=5)\n",
    "X_train_tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf_vector = X_train_tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf_vector = X_train_tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Algorithms\n",
    "\n",
    "Many algorithms exist for machine learning predictions. Some, like the Support Vector Machine and Linear/Multiple Regression, have existed for a long time and are applicable to a wide range of problems, while others are more recent and are meant to be high-performance on more specific problems. Here, we present three algorithms most commonly used to classify text data.\n",
    "\n",
    "### 4.2.1 The Perceptron\n",
    "In 1943, McCulloch & Pitts developed the first Perceptron, called the Neuron Model. This model was first used in 1958 by Frank Rosenblatt, who formed the classical perceptron model, and in 1969, Minsky and Papert published extensive research on the perceptron model. \n",
    "\n",
    "The single perceptron is a binary classifier and takes each input $x_i$ and multiplies it by some corresponding weight $w_i$. These are then added for all $x$ and $w$, or $\\sum_{i=1}^{N} x_iw_i$. This sum is compared to a threshold $T$, producing an output of 0 if below $T$ or 1 if above $T$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sec3_perceptron.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of function is called an activation function, which generally involve a value of 0  or -1 until a threshold $O$ is reached, after which the function stays at 1 or increases. Alternative activation functions include logistic regression (below middle) or the rectified linear unit (ReLU) (below right), with the perceptron at bottom left for reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sec3_activ_funcs.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was proven relatively quickly that in addition to only being a binary classifier, the perceptron can only handle linearly separable data. These problems can be fixed by using a multilayer perceptron. \n",
    "\n",
    "Multilayer perceptrons combine several perceptrons in at least one hidden layer, each layer feeding the results of the neurons to next layer; the components of the final hidden layer then go on to form the output. Unlike single perceptrons, multilayer perceptrons can handle non-linear data and represent arbitrary decision boundaries.\n",
    "\n",
    "To calculate the weights for each layer, we first compute the errors at the output layer with a cost function.  These errors are then distributed backwards from each layer to the previous layer. Then we use gradient descent on the error functions to adjust weight at the layer, so we need all functions differentiable; the weights that minimize the error function form the solution to the weights for the perceptron.\n",
    "\n",
    "Once we have our vectorized data ready to go, we can easily apply $\\texttt{MLPClassifier()}$ to our training set then evaluate it on the testing set. Our libraries already have built-in functions to report various metrics, which are expanded more in the next section. For now, we only need to look at accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mlp_clf = MLPClassifier(random_state=1, max_iter=100).fit(X_train_tfidf_vector, y_train)\n",
    "y_pred = tfidf_mlp_clf.predict(X_test_tfidf_vector)\n",
    "tfidf_mlp_clf.score(X_test_tfidf_vector, y_test)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4.1</b>: Repeat the above classification on unigrams and report the accuracy.\n",
    "\n",
    "### 4.2.2 Naive Bayes Algorithms\n",
    "\n",
    "Naive Bayes Algorithms are so called as they rely on Bayes' Theorem and assume high levels of independence (naive). Bayes' Theorem states that \n",
    "\n",
    "$P(y|X) = \\frac{P(X|y) * P(y)}{P(X)}$.\n",
    "\n",
    "In terms of Bayesian Probability, $P(y|X)$ is called the <i>posterior</i>, $P(X|y)$ is called the <i>data likelihood</i>, $P(y)$ is the <i>prior</i>, and $P(X)$ is the <i>normalization</i>. In the context of NLP classification, we define $X$ to be the document features and $y$ to be the document's class label. We also assume conditional independence, which means that the class label of one document does not depend on the class label of another document.\n",
    "\n",
    "We define $\\underset{a}{\\arg \\max} f$ to be the value of $a$ such that the function $f$ is maximized. Then, Naive Bayes Algorithms classify a document into a class $\\hat{y}$ by calculating\n",
    "\n",
    "$\\hat{y} = \\underset{y}{\\arg \\max} P(y|X) = \\underset{y}{\\arg \\max} P(X|y)*P(y)$.\n",
    "\n",
    "Note that when using Naive Bayes, we have the assumption that features are independent given class. This allows us to compute\n",
    "\n",
    "$P(x_1, x_2, â€¦, x_n |y) = \\Pi_{i=1}^{n} P(x_i|y)$\n",
    "\n",
    "We can estimate either side of this equation with training data, but estimating the right hand side $\\Pi_{i=1}^{n} P(x_i|y)$ leads to a much greater reduction of computation.\n",
    "\n",
    "Similar to the perceptron, we fit the Naive Bayes algorithm to our training and testing set and report metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_multinomialNB_clf = MultinomialNB().fit(X_train_tfidf_vector, y_train)\n",
    "y_pred = tfidf_multinomialNB_clf.predict(X_test_tfidf_vector)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4.2</b>: Repeat the above classifications on unigrams and report the accuracy.\n",
    "\n",
    "### 4.2.3 Ridge Classification\n",
    "\n",
    "Ridge Classification was designed with the original intent of a restriction on basic multiple linear regression; in essence, it adds an additional penalty for extremely large errors, so ridge regression minimizes more than just the least square residuals. However, ridge algorithms can also be used for classification, and in the context of NLP tasks, ridge classifications have been found to perform very well.\n",
    "\n",
    "In the binary case, ridge converts the two classes to either 1 or -1 and then treats the problem as a regression problem. This can then be generalized to multiple classes with a One vs. Rest approach, where we make an algorithm to determine if an item is in one specific class or any of the rest of the classes; this is then repeated until any item can be classified into a single class.\n",
    "\n",
    "More on the ways ridge regression is performed and calculated for classification can be found on the scikit-learn explanation: https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression.\n",
    "\n",
    "Again, similar to previous classification methods, we run a ridge classification as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "tfidf_ridge_clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "tfidf_ridge_clf = tfidf_ridge_clf.fit(X_train_tfidf_vector, y_train)\n",
    "y_pred = tfidf_ridge_clf.predict(X_test_tfidf_vector)\n",
    "print(metrics.classification_report(y_test,y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4.3</b>: Repeat the above classification on unigrams and report the accuracy.\n",
    "\n",
    "## 4.3 More Exercises\n",
    "\n",
    "<b>Exercise 4.4</b>: Write a function that takes a document corpus, processed like in the exercises in Section 1, splits the set into a training and testing set, and runs a classifier on the training set then reports metrics on the testing set.\n",
    " - Your function should take a parameter $\\texttt{classifier\\_algorithm}$ that specifies the algorithm to use as well as the parameters necessary for that algorithm.\n",
    "\n",
    "<b>Exercise 4.5</b>: Run your function on the SDG corpus using Multinomial Naive Bayes, Multilayer Perceptron, and Ridge Regression.\n",
    " - Use $\\texttt{min\\_df}$ to modify the computation time for Multilayer Perceptron; this will reduce the amount of time it takes to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
